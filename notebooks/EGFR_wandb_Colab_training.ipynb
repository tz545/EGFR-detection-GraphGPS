{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Finetuning using OGB hyperparameters on EGFR compounds"
      ],
      "metadata": {
        "id": "f1j-AHx3LD9Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSBPAyeJCv0r",
        "outputId": "2ee02934-99db-447a-ce58-2113755ee808"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/Documents/HLML/EGFR-detection-GraphGPS\n"
          ]
        }
      ],
      "source": [
        "IN_COLAB = 'google.colab' in str(get_ipython())\n",
        "\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    %cd /content/drive/MyDrive/Documents/HLML/EGFR-detection-GraphGPS/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb -Uq\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "_9AM5Xt8DzXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "import torch\n",
        "\n",
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "i3oGc3USD0-p",
        "outputId": "8fb136d3-b89c-417b-c0c9-7fdcc71f4122"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "nsPNIdFJIagT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --cfg configs/GPS/EGFR-finetune.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "932YJreyUGlb",
        "outputId": "9929f8e7-aa56-4d87-a411-8167a090b903"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.\n",
            "[*] Updating cfg from pretrained model: pretrained/pcqm4m-GPS+RWSE.deep/config.yaml\n",
            "[*] Run ID 0: seed=0, split_index=0\n",
            "    Starting now: 2023-09-03 20:29:04.062747\n",
            "[*] Loaded dataset 'EGFR_compounds_lipinsky.csv' from 'OGB-EGFR':\n",
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
            "  warnings.warn(msg)\n",
            "  Data(x=[134268, 9], edge_index=[2, 294146], edge_attr=[294146, 3], y=[4635])\n",
            "  undirected: True\n",
            "  num graphs: 4635\n",
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
            "  warnings.warn(msg)\n",
            "  avg num_nodes/graph: 28\n",
            "  num node features: 9\n",
            "  num edge features: 3\n",
            "  num classes: (appears to be a regression task)\n",
            "Parsed RWSE PE kernel times / steps: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
            "Precomputing Positional Encoding statistics: ['RWSE'] for all graphs...\n",
            "  ...estimated to be undirected: True\n",
            "100% 4635/4635 [00:06<00:00, 715.06it/s]\n",
            "Done! Took 00:00:06.66\n",
            "[*] Loading from pretrained model: pretrained/pcqm4m-GPS+RWSE.deep/0/ckpt/148.ckpt\n",
            "GraphGymModule(\n",
            "  (model): GPSModel(\n",
            "    (encoder): FeatureEncoder(\n",
            "      (node_encoder): Concat2NodeEncoder(\n",
            "        (encoder1): AtomEncoder(\n",
            "          (atom_embedding_list): ModuleList(\n",
            "            (0): Embedding(119, 236)\n",
            "            (1): Embedding(4, 236)\n",
            "            (2-3): 2 x Embedding(12, 236)\n",
            "            (4): Embedding(10, 236)\n",
            "            (5-6): 2 x Embedding(6, 236)\n",
            "            (7-8): 2 x Embedding(2, 236)\n",
            "          )\n",
            "        )\n",
            "        (encoder2): RWSENodeEncoder(\n",
            "          (raw_norm): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (pe_encoder): Linear(in_features=16, out_features=20, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (edge_encoder): BondEncoder(\n",
            "        (bond_embedding_list): ModuleList(\n",
            "          (0): Embedding(5, 256)\n",
            "          (1): Embedding(6, 256)\n",
            "          (2): Embedding(2, 256)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (layers): Sequential(\n",
            "      (0): GPSLayer(\n",
            "        summary: dim_h=256, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=8\n",
            "        (local_model): GatedGCNLayer()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (norm1_local): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (norm1_attn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (dropout_local): Dropout(p=0.1, inplace=False)\n",
            "        (dropout_attn): Dropout(p=0.1, inplace=False)\n",
            "        (ff_linear1): Linear(in_features=256, out_features=512, bias=True)\n",
            "        (ff_linear2): Linear(in_features=512, out_features=256, bias=True)\n",
            "        (act_fn_ff): GELU(approximate='none')\n",
            "        (norm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (ff_dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (ff_dropout2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (1): GPSLayer(\n",
            "        summary: dim_h=256, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=8\n",
            "        (local_model): GatedGCNLayer()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (norm1_local): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (norm1_attn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (dropout_local): Dropout(p=0.1, inplace=False)\n",
            "        (dropout_attn): Dropout(p=0.1, inplace=False)\n",
            "        (ff_linear1): Linear(in_features=256, out_features=512, bias=True)\n",
            "        (ff_linear2): Linear(in_features=512, out_features=256, bias=True)\n",
            "        (act_fn_ff): GELU(approximate='none')\n",
            "        (norm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (ff_dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (ff_dropout2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (2): GPSLayer(\n",
            "        summary: dim_h=256, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=8\n",
            "        (local_model): GatedGCNLayer()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (norm1_local): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (norm1_attn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (dropout_local): Dropout(p=0.1, inplace=False)\n",
            "        (dropout_attn): Dropout(p=0.1, inplace=False)\n",
            "        (ff_linear1): Linear(in_features=256, out_features=512, bias=True)\n",
            "        (ff_linear2): Linear(in_features=512, out_features=256, bias=True)\n",
            "        (act_fn_ff): GELU(approximate='none')\n",
            "        (norm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (ff_dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (ff_dropout2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (3): GPSLayer(\n",
            "        summary: dim_h=256, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=8\n",
            "        (local_model): GatedGCNLayer()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (norm1_local): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (norm1_attn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (dropout_local): Dropout(p=0.1, inplace=False)\n",
            "        (dropout_attn): Dropout(p=0.1, inplace=False)\n",
            "        (ff_linear1): Linear(in_features=256, out_features=512, bias=True)\n",
            "        (ff_linear2): Linear(in_features=512, out_features=256, bias=True)\n",
            "        (act_fn_ff): GELU(approximate='none')\n",
            "        (norm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (ff_dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (ff_dropout2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (4): GPSLayer(\n",
            "        summary: dim_h=256, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=8\n",
            "        (local_model): GatedGCNLayer()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (norm1_local): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (norm1_attn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (dropout_local): Dropout(p=0.1, inplace=False)\n",
            "        (dropout_attn): Dropout(p=0.1, inplace=False)\n",
            "        (ff_linear1): Linear(in_features=256, out_features=512, bias=True)\n",
            "        (ff_linear2): Linear(in_features=512, out_features=256, bias=True)\n",
            "        (act_fn_ff): GELU(approximate='none')\n",
            "        (norm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (ff_dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (ff_dropout2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (5): GPSLayer(\n",
            "        summary: dim_h=256, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=8\n",
            "        (local_model): GatedGCNLayer()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (norm1_local): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (norm1_attn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (dropout_local): Dropout(p=0.1, inplace=False)\n",
            "        (dropout_attn): Dropout(p=0.1, inplace=False)\n",
            "        (ff_linear1): Linear(in_features=256, out_features=512, bias=True)\n",
            "        (ff_linear2): Linear(in_features=512, out_features=256, bias=True)\n",
            "        (act_fn_ff): GELU(approximate='none')\n",
            "        (norm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (ff_dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (ff_dropout2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (6): GPSLayer(\n",
            "        summary: dim_h=256, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=8\n",
            "        (local_model): GatedGCNLayer()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (norm1_local): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (norm1_attn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (dropout_local): Dropout(p=0.1, inplace=False)\n",
            "        (dropout_attn): Dropout(p=0.1, inplace=False)\n",
            "        (ff_linear1): Linear(in_features=256, out_features=512, bias=True)\n",
            "        (ff_linear2): Linear(in_features=512, out_features=256, bias=True)\n",
            "        (act_fn_ff): GELU(approximate='none')\n",
            "        (norm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (ff_dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (ff_dropout2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (7): GPSLayer(\n",
            "        summary: dim_h=256, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=8\n",
            "        (local_model): GatedGCNLayer()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (norm1_local): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (norm1_attn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (dropout_local): Dropout(p=0.1, inplace=False)\n",
            "        (dropout_attn): Dropout(p=0.1, inplace=False)\n",
            "        (ff_linear1): Linear(in_features=256, out_features=512, bias=True)\n",
            "        (ff_linear2): Linear(in_features=512, out_features=256, bias=True)\n",
            "        (act_fn_ff): GELU(approximate='none')\n",
            "        (norm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (ff_dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (ff_dropout2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (8): GPSLayer(\n",
            "        summary: dim_h=256, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=8\n",
            "        (local_model): GatedGCNLayer()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (norm1_local): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (norm1_attn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (dropout_local): Dropout(p=0.1, inplace=False)\n",
            "        (dropout_attn): Dropout(p=0.1, inplace=False)\n",
            "        (ff_linear1): Linear(in_features=256, out_features=512, bias=True)\n",
            "        (ff_linear2): Linear(in_features=512, out_features=256, bias=True)\n",
            "        (act_fn_ff): GELU(approximate='none')\n",
            "        (norm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (ff_dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (ff_dropout2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (9): GPSLayer(\n",
            "        summary: dim_h=256, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=8\n",
            "        (local_model): GatedGCNLayer()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (norm1_local): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (norm1_attn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (dropout_local): Dropout(p=0.1, inplace=False)\n",
            "        (dropout_attn): Dropout(p=0.1, inplace=False)\n",
            "        (ff_linear1): Linear(in_features=256, out_features=512, bias=True)\n",
            "        (ff_linear2): Linear(in_features=512, out_features=256, bias=True)\n",
            "        (act_fn_ff): GELU(approximate='none')\n",
            "        (norm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (ff_dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (ff_dropout2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (10): GPSLayer(\n",
            "        summary: dim_h=256, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=8\n",
            "        (local_model): GatedGCNLayer()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (norm1_local): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (norm1_attn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (dropout_local): Dropout(p=0.1, inplace=False)\n",
            "        (dropout_attn): Dropout(p=0.1, inplace=False)\n",
            "        (ff_linear1): Linear(in_features=256, out_features=512, bias=True)\n",
            "        (ff_linear2): Linear(in_features=512, out_features=256, bias=True)\n",
            "        (act_fn_ff): GELU(approximate='none')\n",
            "        (norm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (ff_dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (ff_dropout2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (11): GPSLayer(\n",
            "        summary: dim_h=256, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=8\n",
            "        (local_model): GatedGCNLayer()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (norm1_local): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (norm1_attn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (dropout_local): Dropout(p=0.1, inplace=False)\n",
            "        (dropout_attn): Dropout(p=0.1, inplace=False)\n",
            "        (ff_linear1): Linear(in_features=256, out_features=512, bias=True)\n",
            "        (ff_linear2): Linear(in_features=512, out_features=256, bias=True)\n",
            "        (act_fn_ff): GELU(approximate='none')\n",
            "        (norm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (ff_dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (ff_dropout2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (12): GPSLayer(\n",
            "        summary: dim_h=256, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=8\n",
            "        (local_model): GatedGCNLayer()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (norm1_local): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (norm1_attn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (dropout_local): Dropout(p=0.1, inplace=False)\n",
            "        (dropout_attn): Dropout(p=0.1, inplace=False)\n",
            "        (ff_linear1): Linear(in_features=256, out_features=512, bias=True)\n",
            "        (ff_linear2): Linear(in_features=512, out_features=256, bias=True)\n",
            "        (act_fn_ff): GELU(approximate='none')\n",
            "        (norm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (ff_dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (ff_dropout2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (13): GPSLayer(\n",
            "        summary: dim_h=256, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=8\n",
            "        (local_model): GatedGCNLayer()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (norm1_local): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (norm1_attn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (dropout_local): Dropout(p=0.1, inplace=False)\n",
            "        (dropout_attn): Dropout(p=0.1, inplace=False)\n",
            "        (ff_linear1): Linear(in_features=256, out_features=512, bias=True)\n",
            "        (ff_linear2): Linear(in_features=512, out_features=256, bias=True)\n",
            "        (act_fn_ff): GELU(approximate='none')\n",
            "        (norm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (ff_dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (ff_dropout2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (14): GPSLayer(\n",
            "        summary: dim_h=256, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=8\n",
            "        (local_model): GatedGCNLayer()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (norm1_local): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (norm1_attn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (dropout_local): Dropout(p=0.1, inplace=False)\n",
            "        (dropout_attn): Dropout(p=0.1, inplace=False)\n",
            "        (ff_linear1): Linear(in_features=256, out_features=512, bias=True)\n",
            "        (ff_linear2): Linear(in_features=512, out_features=256, bias=True)\n",
            "        (act_fn_ff): GELU(approximate='none')\n",
            "        (norm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (ff_dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (ff_dropout2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (15): GPSLayer(\n",
            "        summary: dim_h=256, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=8\n",
            "        (local_model): GatedGCNLayer()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (norm1_local): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (norm1_attn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (dropout_local): Dropout(p=0.1, inplace=False)\n",
            "        (dropout_attn): Dropout(p=0.1, inplace=False)\n",
            "        (ff_linear1): Linear(in_features=256, out_features=512, bias=True)\n",
            "        (ff_linear2): Linear(in_features=512, out_features=256, bias=True)\n",
            "        (act_fn_ff): GELU(approximate='none')\n",
            "        (norm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (ff_dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (ff_dropout2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (post_mp): SANGraphHead(\n",
            "      (FC_layers): ModuleList(\n",
            "        (0): Linear(in_features=256, out_features=128, bias=True)\n",
            "        (1): Linear(in_features=128, out_features=64, bias=True)\n",
            "        (2): Linear(in_features=64, out_features=1, bias=True)\n",
            "      )\n",
            "      (activation): GELU(approximate='none')\n",
            "    )\n",
            "  )\n",
            ")\n",
            "accelerator: cuda\n",
            "benchmark: False\n",
            "bn:\n",
            "  eps: 1e-05\n",
            "  mom: 0.1\n",
            "cfg_dest: config.yaml\n",
            "custom_metrics: []\n",
            "dataset:\n",
            "  cache_load: False\n",
            "  cache_save: False\n",
            "  dir: ./datasets\n",
            "  edge_dim: 128\n",
            "  edge_encoder: True\n",
            "  edge_encoder_bn: False\n",
            "  edge_encoder_name: Bond\n",
            "  edge_encoder_num_types: 0\n",
            "  edge_message_ratio: 0.8\n",
            "  edge_negative_sampling_ratio: 1.0\n",
            "  edge_train_mode: all\n",
            "  encoder: True\n",
            "  encoder_bn: True\n",
            "  encoder_dim: 128\n",
            "  encoder_name: db\n",
            "  format: OGB-EGFR\n",
            "  infer_link_label: None\n",
            "  label_column: none\n",
            "  label_table: none\n",
            "  location: local\n",
            "  name: EGFR_compounds_lipinsky.csv\n",
            "  node_encoder: True\n",
            "  node_encoder_bn: False\n",
            "  node_encoder_name: Atom+RWSE\n",
            "  node_encoder_num_types: 0\n",
            "  remove_feature: False\n",
            "  resample_disjoint: False\n",
            "  resample_negative: False\n",
            "  shuffle_split: True\n",
            "  slic_compactness: 10\n",
            "  split: [0.7, 0.2, 0.1]\n",
            "  split_dir: ./splits\n",
            "  split_index: 0\n",
            "  split_mode: random\n",
            "  task: graph\n",
            "  task_type: regression\n",
            "  to_undirected: False\n",
            "  transductive: False\n",
            "  transform: none\n",
            "  tu_simple: True\n",
            "devices: 1\n",
            "example_arg: example\n",
            "example_group:\n",
            "  example_arg: example\n",
            "gnn:\n",
            "  act: gelu\n",
            "  agg: add\n",
            "  att_final_linear: False\n",
            "  att_final_linear_bn: False\n",
            "  att_heads: 1\n",
            "  batchnorm: True\n",
            "  clear_feature: True\n",
            "  dim_edge: 256\n",
            "  dim_inner: 256\n",
            "  dropout: 0.0\n",
            "  head: san_graph\n",
            "  keep_edge: 0.5\n",
            "  l2norm: True\n",
            "  layer_type: generalconv\n",
            "  layers_mp: 2\n",
            "  layers_post_mp: 3\n",
            "  layers_pre_mp: 0\n",
            "  msg_direction: single\n",
            "  normalize_adj: False\n",
            "  residual: False\n",
            "  self_msg: concat\n",
            "  skip_every: 1\n",
            "  stage_type: stack\n",
            "gpu_mem: False\n",
            "graphormer:\n",
            "  attention_dropout: 0.0\n",
            "  dropout: 0.0\n",
            "  embed_dim: 80\n",
            "  input_dropout: 0.0\n",
            "  mlp_dropout: 0.0\n",
            "  num_heads: 4\n",
            "  num_layers: 6\n",
            "  use_graph_token: True\n",
            "gt:\n",
            "  attn_dropout: 0.1\n",
            "  batch_norm: True\n",
            "  bigbird:\n",
            "    add_cross_attention: False\n",
            "    attention_type: block_sparse\n",
            "    block_size: 3\n",
            "    chunk_size_feed_forward: 0\n",
            "    hidden_act: relu\n",
            "    is_decoder: False\n",
            "    layer_norm_eps: 1e-06\n",
            "    max_position_embeddings: 128\n",
            "    num_random_blocks: 3\n",
            "    use_bias: False\n",
            "  dim_hidden: 256\n",
            "  dropout: 0.1\n",
            "  full_graph: True\n",
            "  gamma: 1e-05\n",
            "  layer_norm: False\n",
            "  layer_type: CustomGatedGCN+Transformer\n",
            "  layers: 16\n",
            "  n_heads: 8\n",
            "  pna_degrees: []\n",
            "  residual: True\n",
            "mem:\n",
            "  inplace: False\n",
            "metric_agg: argmin\n",
            "metric_best: mae\n",
            "model:\n",
            "  edge_decoding: dot\n",
            "  graph_pooling: mean\n",
            "  loss_fun: l1\n",
            "  match_upper: True\n",
            "  size_average: mean\n",
            "  thresh: 0.5\n",
            "  type: GPSModel\n",
            "name_tag: \n",
            "num_threads: 6\n",
            "num_workers: 0\n",
            "optim:\n",
            "  base_lr: 0.0002\n",
            "  batch_accumulation: 1\n",
            "  clip_grad_norm: True\n",
            "  clip_grad_norm_value: 1.0\n",
            "  lr_decay: 0.1\n",
            "  max_epoch: 150\n",
            "  min_lr: 0.0\n",
            "  momentum: 0.9\n",
            "  num_warmup_epochs: 10\n",
            "  optimizer: adamW\n",
            "  reduce_factor: 0.1\n",
            "  schedule_patience: 10\n",
            "  scheduler: linear_with_warmup\n",
            "  steps: [30, 60, 90]\n",
            "  weight_decay: 0.0\n",
            "out_dir: results/EGFR-finetune\n",
            "posenc_ElstaticSE:\n",
            "  dim_pe: 16\n",
            "  enable: False\n",
            "  kernel:\n",
            "    times: []\n",
            "    times_func: range(10)\n",
            "  layers: 3\n",
            "  model: none\n",
            "  n_heads: 4\n",
            "  pass_as_var: False\n",
            "  post_layers: 0\n",
            "  raw_norm_type: none\n",
            "posenc_EquivStableLapPE:\n",
            "  eigen:\n",
            "    eigvec_norm: L2\n",
            "    laplacian_norm: sym\n",
            "    max_freqs: 10\n",
            "  enable: False\n",
            "  raw_norm_type: none\n",
            "posenc_GraphormerBias:\n",
            "  dim_pe: 0\n",
            "  enable: False\n",
            "  node_degrees_only: False\n",
            "  num_in_degrees: None\n",
            "  num_out_degrees: None\n",
            "  num_spatial_types: None\n",
            "posenc_HKdiagSE:\n",
            "  dim_pe: 16\n",
            "  enable: False\n",
            "  kernel:\n",
            "    times: []\n",
            "    times_func: \n",
            "  layers: 3\n",
            "  model: none\n",
            "  n_heads: 4\n",
            "  pass_as_var: False\n",
            "  post_layers: 0\n",
            "  raw_norm_type: none\n",
            "posenc_LapPE:\n",
            "  dim_pe: 16\n",
            "  eigen:\n",
            "    eigvec_norm: L2\n",
            "    laplacian_norm: sym\n",
            "    max_freqs: 10\n",
            "  enable: False\n",
            "  layers: 3\n",
            "  model: none\n",
            "  n_heads: 4\n",
            "  pass_as_var: False\n",
            "  post_layers: 0\n",
            "  raw_norm_type: none\n",
            "posenc_RWSE:\n",
            "  dim_pe: 20\n",
            "  enable: True\n",
            "  kernel:\n",
            "    times: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
            "    times_func: range(1,17)\n",
            "  layers: 3\n",
            "  model: Linear\n",
            "  n_heads: 4\n",
            "  pass_as_var: False\n",
            "  post_layers: 0\n",
            "  raw_norm_type: BatchNorm\n",
            "posenc_SignNet:\n",
            "  dim_pe: 16\n",
            "  eigen:\n",
            "    eigvec_norm: L2\n",
            "    laplacian_norm: sym\n",
            "    max_freqs: 10\n",
            "  enable: False\n",
            "  layers: 3\n",
            "  model: none\n",
            "  n_heads: 4\n",
            "  pass_as_var: False\n",
            "  phi_hidden_dim: 64\n",
            "  phi_out_dim: 4\n",
            "  post_layers: 0\n",
            "  raw_norm_type: none\n",
            "pretrained:\n",
            "  dir: pretrained/pcqm4m-GPS+RWSE.deep\n",
            "  freeze_main: False\n",
            "  reset_prediction_head: False\n",
            "print: both\n",
            "round: 5\n",
            "run_dir: results/EGFR-finetune/0\n",
            "run_id: 0\n",
            "run_multiple_splits: []\n",
            "seed: 0\n",
            "share:\n",
            "  dim_in: 9\n",
            "  dim_out: 1\n",
            "  num_splits: 3\n",
            "tensorboard_agg: True\n",
            "tensorboard_each_run: False\n",
            "train:\n",
            "  auto_resume: False\n",
            "  batch_size: 32\n",
            "  ckpt_best: False\n",
            "  ckpt_clean: True\n",
            "  ckpt_period: 100\n",
            "  enable_ckpt: True\n",
            "  epoch_resume: -1\n",
            "  eval_period: 1\n",
            "  iter_per_epoch: 32\n",
            "  mode: custom\n",
            "  neighbor_sizes: [20, 15, 10, 5]\n",
            "  node_per_graph: 32\n",
            "  radius: extend\n",
            "  sample_node: False\n",
            "  sampler: full_batch\n",
            "  skip_train_eval: False\n",
            "  walk_length: 4\n",
            "val:\n",
            "  node_per_graph: 32\n",
            "  radius: extend\n",
            "  sample_node: False\n",
            "  sampler: full_batch\n",
            "view_emb: False\n",
            "wandb:\n",
            "  entity: tz545\n",
            "  name: \n",
            "  project: EGFR\n",
            "  use: True\n",
            "Num parameters: 13807345\n",
            "Start from epoch 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtz545\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/Documents/HLML/EGFR-detection-GraphGPS/wandb/run-20230903_202925-e0uhatbt\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m-EGFR-EGFR_compounds_lipinsky.csv.GPS.CustomGatedGCN+Transformer+RWSE.r0\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/tz545/EGFR\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/tz545/EGFR/runs/e0uhatbt\u001b[0m\n",
            "Key posenc_GraphormerBias.num_spatial_types with value <class 'NoneType'> is not a valid type; valid types: {<class 'float'>, <class 'bool'>, <class 'list'>, <class 'int'>, <class 'tuple'>, <class 'str'>}\n",
            "Key posenc_GraphormerBias.num_in_degrees with value <class 'NoneType'> is not a valid type; valid types: {<class 'float'>, <class 'bool'>, <class 'list'>, <class 'int'>, <class 'tuple'>, <class 'str'>}\n",
            "Key posenc_GraphormerBias.num_out_degrees with value <class 'NoneType'> is not a valid type; valid types: {<class 'float'>, <class 'bool'>, <class 'list'>, <class 'int'>, <class 'tuple'>, <class 'str'>}\n",
            "train: {'epoch': 0, 'time_epoch': 16.88946, 'eta': 2516.53016, 'eta_hours': 0.69904, 'loss': 1.61642013, 'lr': 0.0, 'params': 13807345, 'time_iter': 0.16558, 'mae': 1.61642, 'r2': -0.93988, 'spearmanr': 0.02899, 'mse': 3.98537, 'rmse': 1.99634}\n",
            "...computing epoch stats took: 0.01s\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 0, 'time_epoch': 1.41028, 'loss': 1.71228389, 'lr': 0, 'params': 13807345, 'time_iter': 0.04863, 'mae': 1.71228, 'r2': -1.14472, 'spearmanr': 0.02797, 'mse': 4.34503, 'rmse': 2.08447}\n",
            "...computing epoch stats took: 0.01s\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 0, 'time_epoch': 0.99779, 'loss': 1.72884907, 'lr': 0, 'params': 13807345, 'time_iter': 0.06652, 'mae': 1.72885, 'r2': -1.19357, 'spearmanr': 0.03666, 'mse': 4.33965, 'rmse': 2.08318}\n",
            "...computing epoch stats took: 0.01s\n",
            "> Epoch 0: took 19.3s (avg 19.3s) | Best so far: epoch 0\ttrain_loss: 1.6164 train_mae: 1.6164\tval_loss: 1.7123 val_mae: 1.7123\ttest_loss: 1.7288 test_mae: 1.7288\n",
            "train: {'epoch': 1, 'time_epoch': 14.38331, 'eta': 2314.18524, 'eta_hours': 0.64283, 'loss': 1.5117546, 'lr': 2e-05, 'params': 13807345, 'time_iter': 0.14101, 'mae': 1.51175, 'r2': -0.69767, 'spearmanr': 0.21762, 'mse': 3.48775, 'rmse': 1.86755}\n",
            "...computing epoch stats took: 0.01s\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 1, 'time_epoch': 1.32061, 'loss': 1.43147301, 'lr': 0, 'params': 13807345, 'time_iter': 0.04554, 'mae': 1.43147, 'r2': -0.53718, 'spearmanr': 0.38725, 'mse': 3.1142, 'rmse': 1.76471}\n",
            "...computing epoch stats took: 0.01s\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 1, 'time_epoch': 0.69962, 'loss': 1.45683871, 'lr': 0, 'params': 13807345, 'time_iter': 0.04664, 'mae': 1.45684, 'r2': -0.59873, 'spearmanr': 0.38897, 'mse': 3.16284, 'rmse': 1.77844}\n",
            "...computing epoch stats took: 0.01s\n",
            "> Epoch 1: took 16.4s (avg 17.9s) | Best so far: epoch 1\ttrain_loss: 1.5118 train_mae: 1.5117\tval_loss: 1.4315 val_mae: 1.4315\ttest_loss: 1.4568 test_mae: 1.4568\n",
            "train: {'epoch': 2, 'time_epoch': 14.92098, 'eta': 2263.49382, 'eta_hours': 0.62875, 'loss': 1.23279356, 'lr': 4e-05, 'params': 13807345, 'time_iter': 0.14628, 'mae': 1.23279, 'r2': -0.16526, 'spearmanr': 0.47691, 'mse': 2.39396, 'rmse': 1.54724}\n",
            "...computing epoch stats took: 0.01s\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 2, 'time_epoch': 1.37579, 'loss': 1.03922714, 'lr': 0, 'params': 13807345, 'time_iter': 0.04744, 'mae': 1.03923, 'r2': 0.15184, 'spearmanr': 0.61476, 'mse': 1.7183, 'rmse': 1.31084}\n",
            "...computing epoch stats took: 0.01s\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 2, 'time_epoch': 0.6907, 'loss': 1.04198053, 'lr': 0, 'params': 13807345, 'time_iter': 0.04605, 'mae': 1.04198, 'r2': 0.11995, 'spearmanr': 0.60689, 'mse': 1.74105, 'rmse': 1.31949}\n",
            "...computing epoch stats took: 0.01s\n",
            "> Epoch 2: took 17.0s (avg 17.6s) | Best so far: epoch 2\ttrain_loss: 1.2328 train_mae: 1.2328\tval_loss: 1.0392 val_mae: 1.0392\ttest_loss: 1.0420 test_mae: 1.0420\n",
            "train: {'epoch': 3, 'time_epoch': 14.53936, 'eta': 2216.75842, 'eta_hours': 0.61577, 'loss': 0.88360455, 'lr': 6e-05, 'params': 13807345, 'time_iter': 0.14254, 'mae': 0.8836, 'r2': 0.38431, 'spearmanr': 0.65905, 'mse': 1.26489, 'rmse': 1.12467}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 3, 'time_epoch': 1.35392, 'loss': 0.8010478, 'lr': 0, 'params': 13807345, 'time_iter': 0.04669, 'mae': 0.80105, 'r2': 0.47015, 'spearmanr': 0.71852, 'mse': 1.07344, 'rmse': 1.03607}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 3, 'time_epoch': 0.68747, 'loss': 0.74828185, 'lr': 0, 'params': 13807345, 'time_iter': 0.04583, 'mae': 0.74828, 'r2': 0.51036, 'spearmanr': 0.72209, 'mse': 0.96868, 'rmse': 0.98421}\n",
            "> Epoch 3: took 16.6s (avg 17.4s) | Best so far: epoch 3\ttrain_loss: 0.8836 train_mae: 0.8836\tval_loss: 0.8010 val_mae: 0.8011\ttest_loss: 0.7483 test_mae: 0.7483\n",
            "train: {'epoch': 4, 'time_epoch': 14.44047, 'eta': 2180.03382, 'eta_hours': 0.60556, 'loss': 0.74869823, 'lr': 8e-05, 'params': 13807345, 'time_iter': 0.14157, 'mae': 0.7487, 'r2': 0.53992, 'spearmanr': 0.74154, 'mse': 0.9452, 'rmse': 0.97222}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 4, 'time_epoch': 1.34129, 'loss': 0.7269531, 'lr': 0, 'params': 13807345, 'time_iter': 0.04625, 'mae': 0.72695, 'r2': 0.54824, 'spearmanr': 0.77243, 'mse': 0.91522, 'rmse': 0.95667}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 4, 'time_epoch': 0.72152, 'loss': 0.69652366, 'lr': 0, 'params': 13807345, 'time_iter': 0.0481, 'mae': 0.69652, 'r2': 0.58375, 'spearmanr': 0.77321, 'mse': 0.82348, 'rmse': 0.90746}\n",
            "> Epoch 4: took 16.5s (avg 17.2s) | Best so far: epoch 4\ttrain_loss: 0.7487 train_mae: 0.7487\tval_loss: 0.7270 val_mae: 0.7269\ttest_loss: 0.6965 test_mae: 0.6965\n",
            "train: {'epoch': 5, 'time_epoch': 14.03145, 'eta': 2140.92074, 'eta_hours': 0.5947, 'loss': 0.68394124, 'lr': 0.0001, 'params': 13807345, 'time_iter': 0.13756, 'mae': 0.68394, 'r2': 0.60608, 'spearmanr': 0.78708, 'mse': 0.80928, 'rmse': 0.8996}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 5, 'time_epoch': 1.96242, 'loss': 0.67969439, 'lr': 0, 'params': 13807345, 'time_iter': 0.06767, 'mae': 0.67969, 'r2': 0.58886, 'spearmanr': 0.7909, 'mse': 0.83293, 'rmse': 0.91265}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 5, 'time_epoch': 0.6982, 'loss': 0.65318961, 'lr': 0, 'params': 13807345, 'time_iter': 0.04655, 'mae': 0.65319, 'r2': 0.61699, 'spearmanr': 0.78589, 'mse': 0.75773, 'rmse': 0.87048}\n",
            "> Epoch 5: took 16.7s (avg 17.1s) | Best so far: epoch 5\ttrain_loss: 0.6839 train_mae: 0.6839\tval_loss: 0.6797 val_mae: 0.6797\ttest_loss: 0.6532 test_mae: 0.6532\n",
            "train: {'epoch': 6, 'time_epoch': 12.72184, 'eta': 2082.22039, 'eta_hours': 0.57839, 'loss': 0.65108654, 'lr': 0.00012, 'params': 13807345, 'time_iter': 0.12472, 'mae': 0.65109, 'r2': 0.64466, 'spearmanr': 0.80758, 'mse': 0.73003, 'rmse': 0.85442}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 6, 'time_epoch': 1.97214, 'loss': 0.67909435, 'lr': 0, 'params': 13807345, 'time_iter': 0.068, 'mae': 0.67909, 'r2': 0.57717, 'spearmanr': 0.79809, 'mse': 0.85662, 'rmse': 0.92554}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 6, 'time_epoch': 1.02242, 'loss': 0.66304106, 'lr': 0, 'params': 13807345, 'time_iter': 0.06816, 'mae': 0.66304, 'r2': 0.5881, 'spearmanr': 0.79017, 'mse': 0.81487, 'rmse': 0.9027}\n",
            "> Epoch 6: took 15.8s (avg 16.9s) | Best so far: epoch 6\ttrain_loss: 0.6511 train_mae: 0.6511\tval_loss: 0.6791 val_mae: 0.6791\ttest_loss: 0.6630 test_mae: 0.6630\n",
            "train: {'epoch': 7, 'time_epoch': 12.64173, 'eta': 2033.5927, 'eta_hours': 0.56489, 'loss': 0.60980706, 'lr': 0.00014, 'params': 13807345, 'time_iter': 0.12394, 'mae': 0.60981, 'r2': 0.68345, 'spearmanr': 0.82993, 'mse': 0.65032, 'rmse': 0.80643}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 7, 'time_epoch': 1.74684, 'loss': 0.63797762, 'lr': 0, 'params': 13807345, 'time_iter': 0.06024, 'mae': 0.63798, 'r2': 0.62786, 'spearmanr': 0.81102, 'mse': 0.75392, 'rmse': 0.86829}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 7, 'time_epoch': 0.92261, 'loss': 0.62686235, 'lr': 0, 'params': 13807345, 'time_iter': 0.06151, 'mae': 0.62686, 'r2': 0.63526, 'spearmanr': 0.79903, 'mse': 0.72158, 'rmse': 0.84946}\n",
            "> Epoch 7: took 15.4s (avg 16.7s) | Best so far: epoch 7\ttrain_loss: 0.6098 train_mae: 0.6098\tval_loss: 0.6380 val_mae: 0.6380\ttest_loss: 0.6269 test_mae: 0.6269\n",
            "train: {'epoch': 8, 'time_epoch': 13.49521, 'eta': 2006.33299, 'eta_hours': 0.55731, 'loss': 0.57221709, 'lr': 0.00016, 'params': 13807345, 'time_iter': 0.13231, 'mae': 0.57222, 'r2': 0.71542, 'spearmanr': 0.84822, 'mse': 0.58465, 'rmse': 0.76463}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 8, 'time_epoch': 1.32044, 'loss': 0.66741241, 'lr': 0, 'params': 13807345, 'time_iter': 0.04553, 'mae': 0.66741, 'r2': 0.60697, 'spearmanr': 0.81169, 'mse': 0.79626, 'rmse': 0.89233}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 8, 'time_epoch': 0.67349, 'loss': 0.67419408, 'lr': 0, 'params': 13807345, 'time_iter': 0.0449, 'mae': 0.67419, 'r2': 0.60196, 'spearmanr': 0.80009, 'mse': 0.78746, 'rmse': 0.88739}\n",
            "> Epoch 8: took 15.5s (avg 16.6s) | Best so far: epoch 7\ttrain_loss: 0.6098 train_mae: 0.6098\tval_loss: 0.6380 val_mae: 0.6380\ttest_loss: 0.6269 test_mae: 0.6269\n",
            "train: {'epoch': 9, 'time_epoch': 14.64604, 'eta': 1997.93793, 'eta_hours': 0.55498, 'loss': 0.55344033, 'lr': 0.00018, 'params': 13807345, 'time_iter': 0.14359, 'mae': 0.55344, 'r2': 0.72942, 'spearmanr': 0.85879, 'mse': 0.5559, 'rmse': 0.74558}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 9, 'time_epoch': 1.29784, 'loss': 0.64234924, 'lr': 0, 'params': 13807345, 'time_iter': 0.04475, 'mae': 0.64235, 'r2': 0.63931, 'spearmanr': 0.81037, 'mse': 0.73072, 'rmse': 0.85482}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 9, 'time_epoch': 0.68317, 'loss': 0.62393012, 'lr': 0, 'params': 13807345, 'time_iter': 0.04554, 'mae': 0.62393, 'r2': 0.64712, 'spearmanr': 0.80243, 'mse': 0.69811, 'rmse': 0.83553}\n",
            "> Epoch 9: took 16.7s (avg 16.6s) | Best so far: epoch 7\ttrain_loss: 0.6098 train_mae: 0.6098\tval_loss: 0.6380 val_mae: 0.6380\ttest_loss: 0.6269 test_mae: 0.6269\n",
            "train: {'epoch': 10, 'time_epoch': 14.5342, 'eta': 1986.993, 'eta_hours': 0.55194, 'loss': 0.54917471, 'lr': 0.0002, 'params': 13807345, 'time_iter': 0.14249, 'mae': 0.54917, 'r2': 0.73745, 'spearmanr': 0.86261, 'mse': 0.5394, 'rmse': 0.73444}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 10, 'time_epoch': 1.31261, 'loss': 0.64024809, 'lr': 0, 'params': 13807345, 'time_iter': 0.04526, 'mae': 0.64025, 'r2': 0.62596, 'spearmanr': 0.80973, 'mse': 0.75777, 'rmse': 0.8705}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 10, 'time_epoch': 0.68621, 'loss': 0.61321432, 'lr': 0, 'params': 13807345, 'time_iter': 0.04575, 'mae': 0.61321, 'r2': 0.66588, 'spearmanr': 0.82699, 'mse': 0.661, 'rmse': 0.81302}\n",
            "> Epoch 10: took 16.6s (avg 16.6s) | Best so far: epoch 7\ttrain_loss: 0.6098 train_mae: 0.6098\tval_loss: 0.6380 val_mae: 0.6380\ttest_loss: 0.6269 test_mae: 0.6269\n",
            "train: {'epoch': 11, 'time_epoch': 14.44934, 'eta': 1974.47398, 'eta_hours': 0.54846, 'loss': 0.52190601, 'lr': 0.00019857, 'params': 13807345, 'time_iter': 0.14166, 'mae': 0.52191, 'r2': 0.75788, 'spearmanr': 0.8723, 'mse': 0.49743, 'rmse': 0.70529}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 11, 'time_epoch': 1.34417, 'loss': 0.64397946, 'lr': 0, 'params': 13807345, 'time_iter': 0.04635, 'mae': 0.64398, 'r2': 0.62788, 'spearmanr': 0.81595, 'mse': 0.75388, 'rmse': 0.86826}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 11, 'time_epoch': 0.69201, 'loss': 0.64260307, 'lr': 0, 'params': 13807345, 'time_iter': 0.04613, 'mae': 0.6426, 'r2': 0.63677, 'spearmanr': 0.81544, 'mse': 0.71859, 'rmse': 0.8477}\n",
            "> Epoch 11: took 16.5s (avg 16.6s) | Best so far: epoch 7\ttrain_loss: 0.6098 train_mae: 0.6098\tval_loss: 0.6380 val_mae: 0.6380\ttest_loss: 0.6269 test_mae: 0.6269\n",
            "train: {'epoch': 12, 'time_epoch': 14.30773, 'eta': 1960.16566, 'eta_hours': 0.54449, 'loss': 0.51040285, 'lr': 0.00019714, 'params': 13807345, 'time_iter': 0.14027, 'mae': 0.5104, 'r2': 0.76878, 'spearmanr': 0.88119, 'mse': 0.47504, 'rmse': 0.68923}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 12, 'time_epoch': 1.44482, 'loss': 0.62444852, 'lr': 0, 'params': 13807345, 'time_iter': 0.04982, 'mae': 0.62445, 'r2': 0.63843, 'spearmanr': 0.81954, 'mse': 0.73252, 'rmse': 0.85587}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 12, 'time_epoch': 0.6804, 'loss': 0.61773707, 'lr': 0, 'params': 13807345, 'time_iter': 0.04536, 'mae': 0.61774, 'r2': 0.65245, 'spearmanr': 0.8201, 'mse': 0.68758, 'rmse': 0.82921}\n",
            "> Epoch 12: took 16.5s (avg 16.6s) | Best so far: epoch 12\ttrain_loss: 0.5104 train_mae: 0.5104\tval_loss: 0.6244 val_mae: 0.6244\ttest_loss: 0.6177 test_mae: 0.6177\n",
            "train: {'epoch': 13, 'time_epoch': 13.51718, 'eta': 1938.1778, 'eta_hours': 0.53838, 'loss': 0.4977203, 'lr': 0.00019571, 'params': 13807345, 'time_iter': 0.13252, 'mae': 0.49772, 'r2': 0.77931, 'spearmanr': 0.88755, 'mse': 0.45339, 'rmse': 0.67334}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 13, 'time_epoch': 1.99155, 'loss': 0.62357703, 'lr': 0, 'params': 13807345, 'time_iter': 0.06867, 'mae': 0.62358, 'r2': 0.64833, 'spearmanr': 0.82737, 'mse': 0.71246, 'rmse': 0.84407}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 13, 'time_epoch': 1.02221, 'loss': 0.59133584, 'lr': 0, 'params': 13807345, 'time_iter': 0.06815, 'mae': 0.59134, 'r2': 0.68269, 'spearmanr': 0.83411, 'mse': 0.62775, 'rmse': 0.79231}\n",
            "> Epoch 13: took 16.6s (avg 16.6s) | Best so far: epoch 13\ttrain_loss: 0.4977 train_mae: 0.4977\tval_loss: 0.6236 val_mae: 0.6236\ttest_loss: 0.5913 test_mae: 0.5913\n",
            "train: {'epoch': 14, 'time_epoch': 12.52644, 'eta': 1908.40273, 'eta_hours': 0.53011, 'loss': 0.48285948, 'lr': 0.00019429, 'params': 13807345, 'time_iter': 0.12281, 'mae': 0.48286, 'r2': 0.79204, 'spearmanr': 0.89311, 'mse': 0.42724, 'rmse': 0.65364}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 14, 'time_epoch': 1.85547, 'loss': 0.61229939, 'lr': 0, 'params': 13807345, 'time_iter': 0.06398, 'mae': 0.6123, 'r2': 0.64409, 'spearmanr': 0.8249, 'mse': 0.72105, 'rmse': 0.84914}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 14, 'time_epoch': 1.07752, 'loss': 0.59844874, 'lr': 0, 'params': 13807345, 'time_iter': 0.07183, 'mae': 0.59845, 'r2': 0.67175, 'spearmanr': 0.8278, 'mse': 0.64939, 'rmse': 0.80585}\n",
            "> Epoch 14: took 15.5s (avg 16.5s) | Best so far: epoch 14\ttrain_loss: 0.4829 train_mae: 0.4829\tval_loss: 0.6123 val_mae: 0.6123\ttest_loss: 0.5984 test_mae: 0.5985\n",
            "train: {'epoch': 15, 'time_epoch': 12.83795, 'eta': 1883.39257, 'eta_hours': 0.52316, 'loss': 0.46087515, 'lr': 0.00019286, 'params': 13807345, 'time_iter': 0.12586, 'mae': 0.46088, 'r2': 0.80875, 'spearmanr': 0.90158, 'mse': 0.39291, 'rmse': 0.62683}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 15, 'time_epoch': 1.49074, 'loss': 0.63922448, 'lr': 0, 'params': 13807345, 'time_iter': 0.0514, 'mae': 0.63922, 'r2': 0.62361, 'spearmanr': 0.82253, 'mse': 0.76254, 'rmse': 0.87324}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 15, 'time_epoch': 0.92028, 'loss': 0.62158842, 'lr': 0, 'params': 13807345, 'time_iter': 0.06135, 'mae': 0.62159, 'r2': 0.65569, 'spearmanr': 0.82851, 'mse': 0.68117, 'rmse': 0.82533}\n",
            "> Epoch 15: took 15.3s (avg 16.4s) | Best so far: epoch 14\ttrain_loss: 0.4829 train_mae: 0.4829\tval_loss: 0.6123 val_mae: 0.6123\ttest_loss: 0.5984 test_mae: 0.5985\n",
            "train: {'epoch': 16, 'time_epoch': 14.00452, 'eta': 1868.94112, 'eta_hours': 0.51915, 'loss': 0.46154587, 'lr': 0.00019143, 'params': 13807345, 'time_iter': 0.1373, 'mae': 0.46155, 'r2': 0.80839, 'spearmanr': 0.90259, 'mse': 0.39365, 'rmse': 0.62741}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 16, 'time_epoch': 1.35359, 'loss': 0.62108265, 'lr': 0, 'params': 13807345, 'time_iter': 0.04668, 'mae': 0.62108, 'r2': 0.64698, 'spearmanr': 0.82328, 'mse': 0.71519, 'rmse': 0.84569}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 16, 'time_epoch': 0.68676, 'loss': 0.59555672, 'lr': 0, 'params': 13807345, 'time_iter': 0.04578, 'mae': 0.59556, 'r2': 0.68062, 'spearmanr': 0.83753, 'mse': 0.63185, 'rmse': 0.79489}\n",
            "> Epoch 16: took 16.1s (avg 16.4s) | Best so far: epoch 14\ttrain_loss: 0.4829 train_mae: 0.4829\tval_loss: 0.6123 val_mae: 0.6123\ttest_loss: 0.5984 test_mae: 0.5985\n",
            "train: {'epoch': 17, 'time_epoch': 14.42147, 'eta': 1857.59697, 'eta_hours': 0.516, 'loss': 0.443296, 'lr': 0.00019, 'params': 13807345, 'time_iter': 0.14139, 'mae': 0.4433, 'r2': 0.82335, 'spearmanr': 0.91129, 'mse': 0.36291, 'rmse': 0.60242}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 17, 'time_epoch': 1.31465, 'loss': 0.60098896, 'lr': 0, 'params': 13807345, 'time_iter': 0.04533, 'mae': 0.60099, 'r2': 0.66041, 'spearmanr': 0.83035, 'mse': 0.68797, 'rmse': 0.82944}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 17, 'time_epoch': 0.70477, 'loss': 0.58889357, 'lr': 0, 'params': 13807345, 'time_iter': 0.04698, 'mae': 0.58889, 'r2': 0.68351, 'spearmanr': 0.83402, 'mse': 0.62612, 'rmse': 0.79128}\n",
            "> Epoch 17: took 16.5s (avg 16.4s) | Best so far: epoch 17\ttrain_loss: 0.4433 train_mae: 0.4433\tval_loss: 0.6010 val_mae: 0.6010\ttest_loss: 0.5889 test_mae: 0.5889\n",
            "train: {'epoch': 18, 'time_epoch': 14.54559, 'eta': 1846.78467, 'eta_hours': 0.513, 'loss': 0.44174305, 'lr': 0.00018857, 'params': 13807345, 'time_iter': 0.1426, 'mae': 0.44174, 'r2': 0.82198, 'spearmanr': 0.9106, 'mse': 0.36573, 'rmse': 0.60475}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 18, 'time_epoch': 1.30466, 'loss': 0.61946166, 'lr': 0, 'params': 13807345, 'time_iter': 0.04499, 'mae': 0.61946, 'r2': 0.6347, 'spearmanr': 0.8248, 'mse': 0.74007, 'rmse': 0.86027}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 18, 'time_epoch': 0.67993, 'loss': 0.58632775, 'lr': 0, 'params': 13807345, 'time_iter': 0.04533, 'mae': 0.58633, 'r2': 0.69694, 'spearmanr': 0.84391, 'mse': 0.59955, 'rmse': 0.77431}\n",
            "> Epoch 18: took 16.6s (avg 16.4s) | Best so far: epoch 17\ttrain_loss: 0.4433 train_mae: 0.4433\tval_loss: 0.6010 val_mae: 0.6010\ttest_loss: 0.5889 test_mae: 0.5889\n",
            "train: {'epoch': 19, 'time_epoch': 14.29585, 'eta': 1833.97576, 'eta_hours': 0.50944, 'loss': 0.45377453, 'lr': 0.00018714, 'params': 13807345, 'time_iter': 0.14016, 'mae': 0.45377, 'r2': 0.81289, 'spearmanr': 0.90418, 'mse': 0.38441, 'rmse': 0.62001}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 19, 'time_epoch': 1.33114, 'loss': 0.60197035, 'lr': 0, 'params': 13807345, 'time_iter': 0.0459, 'mae': 0.60197, 'r2': 0.66891, 'spearmanr': 0.8312, 'mse': 0.67075, 'rmse': 0.819}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 19, 'time_epoch': 0.6878, 'loss': 0.56800307, 'lr': 0, 'params': 13807345, 'time_iter': 0.04585, 'mae': 0.568, 'r2': 0.70932, 'spearmanr': 0.84599, 'mse': 0.57506, 'rmse': 0.75833}\n",
            "> Epoch 19: took 16.4s (avg 16.4s) | Best so far: epoch 17\ttrain_loss: 0.4433 train_mae: 0.4433\tval_loss: 0.6010 val_mae: 0.6010\ttest_loss: 0.5889 test_mae: 0.5889\n",
            "train: {'epoch': 20, 'time_epoch': 13.76925, 'eta': 1817.79041, 'eta_hours': 0.50494, 'loss': 0.4276218, 'lr': 0.00018571, 'params': 13807345, 'time_iter': 0.13499, 'mae': 0.42762, 'r2': 0.83475, 'spearmanr': 0.91609, 'mse': 0.33949, 'rmse': 0.58266}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 20, 'time_epoch': 1.95154, 'loss': 0.59662727, 'lr': 0, 'params': 13807345, 'time_iter': 0.06729, 'mae': 0.59663, 'r2': 0.65632, 'spearmanr': 0.82078, 'mse': 0.69627, 'rmse': 0.83443}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 20, 'time_epoch': 0.69663, 'loss': 0.57377309, 'lr': 0, 'params': 13807345, 'time_iter': 0.04644, 'mae': 0.57377, 'r2': 0.70415, 'spearmanr': 0.84219, 'mse': 0.58529, 'rmse': 0.76504}\n",
            "> Epoch 20: took 16.5s (avg 16.4s) | Best so far: epoch 20\ttrain_loss: 0.4276 train_mae: 0.4276\tval_loss: 0.5966 val_mae: 0.5966\ttest_loss: 0.5738 test_mae: 0.5738\n",
            "train: {'epoch': 21, 'time_epoch': 12.67396, 'eta': 1795.4521, 'eta_hours': 0.49874, 'loss': 0.41937231, 'lr': 0.00018429, 'params': 13807345, 'time_iter': 0.12425, 'mae': 0.41937, 'r2': 0.83802, 'spearmanr': 0.91793, 'mse': 0.33277, 'rmse': 0.57686}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 21, 'time_epoch': 1.98813, 'loss': 0.61393887, 'lr': 0, 'params': 13807345, 'time_iter': 0.06856, 'mae': 0.61394, 'r2': 0.63252, 'spearmanr': 0.82108, 'mse': 0.74449, 'rmse': 0.86284}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 21, 'time_epoch': 1.05723, 'loss': 0.59103268, 'lr': 0, 'params': 13807345, 'time_iter': 0.07048, 'mae': 0.59103, 'r2': 0.68318, 'spearmanr': 0.8413, 'mse': 0.62678, 'rmse': 0.7917}\n",
            "> Epoch 21: took 15.8s (avg 16.4s) | Best so far: epoch 20\ttrain_loss: 0.4276 train_mae: 0.4276\tval_loss: 0.5966 val_mae: 0.5966\ttest_loss: 0.5738 test_mae: 0.5738\n",
            "train: {'epoch': 22, 'time_epoch': 13.03335, 'eta': 1775.93862, 'eta_hours': 0.49332, 'loss': 0.41347408, 'lr': 0.00018286, 'params': 13807345, 'time_iter': 0.12778, 'mae': 0.41347, 'r2': 0.84141, 'spearmanr': 0.91962, 'mse': 0.32581, 'rmse': 0.5708}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 22, 'time_epoch': 1.75624, 'loss': 0.61134696, 'lr': 0, 'params': 13807345, 'time_iter': 0.06056, 'mae': 0.61135, 'r2': 0.62194, 'spearmanr': 0.8104, 'mse': 0.76592, 'rmse': 0.87517}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 22, 'time_epoch': 0.9087, 'loss': 0.60278364, 'lr': 0, 'params': 13807345, 'time_iter': 0.06058, 'mae': 0.60278, 'r2': 0.64438, 'spearmanr': 0.8116, 'mse': 0.70355, 'rmse': 0.83878}\n",
            "> Epoch 22: took 15.7s (avg 16.4s) | Best so far: epoch 20\ttrain_loss: 0.4276 train_mae: 0.4276\tval_loss: 0.5966 val_mae: 0.5966\ttest_loss: 0.5738 test_mae: 0.5738\n",
            "train: {'epoch': 23, 'time_epoch': 13.30697, 'eta': 1758.40165, 'eta_hours': 0.48844, 'loss': 0.40617033, 'lr': 0.00018143, 'params': 13807345, 'time_iter': 0.13046, 'mae': 0.40617, 'r2': 0.84491, 'spearmanr': 0.92178, 'mse': 0.31863, 'rmse': 0.56447}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 23, 'time_epoch': 1.3052, 'loss': 0.59894716, 'lr': 0, 'params': 13807345, 'time_iter': 0.04501, 'mae': 0.59895, 'r2': 0.65735, 'spearmanr': 0.82818, 'mse': 0.69417, 'rmse': 0.83317}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 23, 'time_epoch': 0.70945, 'loss': 0.56514694, 'lr': 0, 'params': 13807345, 'time_iter': 0.0473, 'mae': 0.56515, 'r2': 0.70628, 'spearmanr': 0.84569, 'mse': 0.58109, 'rmse': 0.76229}\n",
            "> Epoch 23: took 15.4s (avg 16.3s) | Best so far: epoch 20\ttrain_loss: 0.4276 train_mae: 0.4276\tval_loss: 0.5966 val_mae: 0.5966\ttest_loss: 0.5738 test_mae: 0.5738\n",
            "train: {'epoch': 24, 'time_epoch': 14.37872, 'eta': 1746.56185, 'eta_hours': 0.48516, 'loss': 0.38961996, 'lr': 0.00018, 'params': 13807345, 'time_iter': 0.14097, 'mae': 0.38962, 'r2': 0.85761, 'spearmanr': 0.92796, 'mse': 0.29254, 'rmse': 0.54087}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 24, 'time_epoch': 1.33614, 'loss': 0.59592498, 'lr': 0, 'params': 13807345, 'time_iter': 0.04607, 'mae': 0.59592, 'r2': 0.64126, 'spearmanr': 0.82592, 'mse': 0.72678, 'rmse': 0.85251}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 24, 'time_epoch': 0.71588, 'loss': 0.54231734, 'lr': 0, 'params': 13807345, 'time_iter': 0.04773, 'mae': 0.54232, 'r2': 0.72528, 'spearmanr': 0.85557, 'mse': 0.5435, 'rmse': 0.73722}\n",
            "> Epoch 24: took 16.5s (avg 16.3s) | Best so far: epoch 24\ttrain_loss: 0.3896 train_mae: 0.3896\tval_loss: 0.5959 val_mae: 0.5959\ttest_loss: 0.5423 test_mae: 0.5423\n",
            "train: {'epoch': 25, 'time_epoch': 14.49911, 'eta': 1735.1009, 'eta_hours': 0.48197, 'loss': 0.39247053, 'lr': 0.00017857, 'params': 13807345, 'time_iter': 0.14215, 'mae': 0.39247, 'r2': 0.85735, 'spearmanr': 0.92697, 'mse': 0.29306, 'rmse': 0.54135}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 25, 'time_epoch': 1.33325, 'loss': 0.59746584, 'lr': 0, 'params': 13807345, 'time_iter': 0.04597, 'mae': 0.59747, 'r2': 0.6568, 'spearmanr': 0.82507, 'mse': 0.69529, 'rmse': 0.83384}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 25, 'time_epoch': 0.68999, 'loss': 0.54391656, 'lr': 0, 'params': 13807345, 'time_iter': 0.046, 'mae': 0.54392, 'r2': 0.72362, 'spearmanr': 0.8499, 'mse': 0.54677, 'rmse': 0.73944}\n",
            "> Epoch 25: took 16.6s (avg 16.3s) | Best so far: epoch 24\ttrain_loss: 0.3896 train_mae: 0.3896\tval_loss: 0.5959 val_mae: 0.5959\ttest_loss: 0.5423 test_mae: 0.5423\n",
            "train: {'epoch': 26, 'time_epoch': 14.49282, 'eta': 1723.38627, 'eta_hours': 0.47872, 'loss': 0.37274074, 'lr': 0.00017714, 'params': 13807345, 'time_iter': 0.14209, 'mae': 0.37274, 'r2': 0.86794, 'spearmanr': 0.93417, 'mse': 0.27131, 'rmse': 0.52088}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 26, 'time_epoch': 1.32579, 'loss': 0.61107807, 'lr': 0, 'params': 13807345, 'time_iter': 0.04572, 'mae': 0.61108, 'r2': 0.63497, 'spearmanr': 0.82606, 'mse': 0.73952, 'rmse': 0.85995}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 26, 'time_epoch': 0.67342, 'loss': 0.58172486, 'lr': 0, 'params': 13807345, 'time_iter': 0.04489, 'mae': 0.58172, 'r2': 0.69296, 'spearmanr': 0.84217, 'mse': 0.60744, 'rmse': 0.77938}\n",
            "> Epoch 26: took 16.5s (avg 16.3s) | Best so far: epoch 24\ttrain_loss: 0.3896 train_mae: 0.3896\tval_loss: 0.5959 val_mae: 0.5959\ttest_loss: 0.5423 test_mae: 0.5423\n",
            "train: {'epoch': 27, 'time_epoch': 13.98649, 'eta': 1709.26703, 'eta_hours': 0.4748, 'loss': 0.3812032, 'lr': 0.00017571, 'params': 13807345, 'time_iter': 0.13712, 'mae': 0.3812, 'r2': 0.8635, 'spearmanr': 0.93166, 'mse': 0.28044, 'rmse': 0.52956}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 27, 'time_epoch': 1.53003, 'loss': 0.59638618, 'lr': 0, 'params': 13807345, 'time_iter': 0.05276, 'mae': 0.59639, 'r2': 0.65146, 'spearmanr': 0.83184, 'mse': 0.70611, 'rmse': 0.84031}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 27, 'time_epoch': 0.69658, 'loss': 0.55017039, 'lr': 0, 'params': 13807345, 'time_iter': 0.04644, 'mae': 0.55017, 'r2': 0.71932, 'spearmanr': 0.85857, 'mse': 0.55529, 'rmse': 0.74518}\n",
            "> Epoch 27: took 16.3s (avg 16.3s) | Best so far: epoch 24\ttrain_loss: 0.3896 train_mae: 0.3896\tval_loss: 0.5959 val_mae: 0.5959\ttest_loss: 0.5423 test_mae: 0.5423\n",
            "train: {'epoch': 28, 'time_epoch': 12.94174, 'eta': 1690.79781, 'eta_hours': 0.46967, 'loss': 0.35930589, 'lr': 0.00017429, 'params': 13807345, 'time_iter': 0.12688, 'mae': 0.35931, 'r2': 0.87437, 'spearmanr': 0.93703, 'mse': 0.25811, 'rmse': 0.50804}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 28, 'time_epoch': 2.0148, 'loss': 0.59292476, 'lr': 0, 'params': 13807345, 'time_iter': 0.06948, 'mae': 0.59292, 'r2': 0.65912, 'spearmanr': 0.8306, 'mse': 0.6906, 'rmse': 0.83103}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 28, 'time_epoch': 1.03617, 'loss': 0.54100473, 'lr': 0, 'params': 13807345, 'time_iter': 0.06908, 'mae': 0.541, 'r2': 0.71935, 'spearmanr': 0.85435, 'mse': 0.55523, 'rmse': 0.74513}\n",
            "> Epoch 28: took 16.0s (avg 16.3s) | Best so far: epoch 28\ttrain_loss: 0.3593 train_mae: 0.3593\tval_loss: 0.5929 val_mae: 0.5929\ttest_loss: 0.5410 test_mae: 0.5410\n",
            "train: {'epoch': 29, 'time_epoch': 12.35633, 'eta': 1670.35544, 'eta_hours': 0.46399, 'loss': 0.36474549, 'lr': 0.00017286, 'params': 13807345, 'time_iter': 0.12114, 'mae': 0.36475, 'r2': 0.87523, 'spearmanr': 0.93752, 'mse': 0.25633, 'rmse': 0.50629}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 29, 'time_epoch': 1.78775, 'loss': 0.6150386, 'lr': 0, 'params': 13807345, 'time_iter': 0.06165, 'mae': 0.61504, 'r2': 0.63138, 'spearmanr': 0.82553, 'mse': 0.74681, 'rmse': 0.86418}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 29, 'time_epoch': 0.96339, 'loss': 0.55817797, 'lr': 0, 'params': 13807345, 'time_iter': 0.06423, 'mae': 0.55818, 'r2': 0.70738, 'spearmanr': 0.84989, 'mse': 0.5789, 'rmse': 0.76086}\n",
            "> Epoch 29: took 15.2s (avg 16.3s) | Best so far: epoch 28\ttrain_loss: 0.3593 train_mae: 0.3593\tval_loss: 0.5929 val_mae: 0.5929\ttest_loss: 0.5410 test_mae: 0.5410\n",
            "train: {'epoch': 30, 'time_epoch': 13.5485, 'eta': 1655.01116, 'eta_hours': 0.45973, 'loss': 0.35421808, 'lr': 0.00017143, 'params': 13807345, 'time_iter': 0.13283, 'mae': 0.35422, 'r2': 0.88105, 'spearmanr': 0.94073, 'mse': 0.24438, 'rmse': 0.49435}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 30, 'time_epoch': 1.31396, 'loss': 0.59771231, 'lr': 0, 'params': 13807345, 'time_iter': 0.04531, 'mae': 0.59771, 'r2': 0.65055, 'spearmanr': 0.82874, 'mse': 0.70795, 'rmse': 0.8414}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 30, 'time_epoch': 0.91956, 'loss': 0.55799355, 'lr': 0, 'params': 13807345, 'time_iter': 0.0613, 'mae': 0.55799, 'r2': 0.70816, 'spearmanr': 0.8511, 'mse': 0.57736, 'rmse': 0.75984}\n",
            "> Epoch 30: took 15.8s (avg 16.3s) | Best so far: epoch 28\ttrain_loss: 0.3593 train_mae: 0.3593\tval_loss: 0.5929 val_mae: 0.5929\ttest_loss: 0.5410 test_mae: 0.5410\n",
            "train: {'epoch': 31, 'time_epoch': 14.12558, 'eta': 1641.90709, 'eta_hours': 0.45609, 'loss': 0.35404435, 'lr': 0.00017, 'params': 13807345, 'time_iter': 0.13849, 'mae': 0.35404, 'r2': 0.87819, 'spearmanr': 0.93831, 'mse': 0.25026, 'rmse': 0.50026}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 31, 'time_epoch': 1.37147, 'loss': 0.61153401, 'lr': 0, 'params': 13807345, 'time_iter': 0.04729, 'mae': 0.61153, 'r2': 0.64432, 'spearmanr': 0.82807, 'mse': 0.72059, 'rmse': 0.84888}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 31, 'time_epoch': 0.71545, 'loss': 0.55886996, 'lr': 0, 'params': 13807345, 'time_iter': 0.0477, 'mae': 0.55887, 'r2': 0.71688, 'spearmanr': 0.8549, 'mse': 0.56012, 'rmse': 0.74841}\n",
            "> Epoch 31: took 16.3s (avg 16.3s) | Best so far: epoch 28\ttrain_loss: 0.3593 train_mae: 0.3593\tval_loss: 0.5929 val_mae: 0.5929\ttest_loss: 0.5410 test_mae: 0.5410\n",
            "train: {'epoch': 32, 'time_epoch': 14.3566, 'eta': 1629.56019, 'eta_hours': 0.45266, 'loss': 0.35384035, 'lr': 0.00016857, 'params': 13807345, 'time_iter': 0.14075, 'mae': 0.35384, 'r2': 0.87976, 'spearmanr': 0.93967, 'mse': 0.24703, 'rmse': 0.49702}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 32, 'time_epoch': 1.35514, 'loss': 0.60050685, 'lr': 0, 'params': 13807345, 'time_iter': 0.04673, 'mae': 0.60051, 'r2': 0.65454, 'spearmanr': 0.82743, 'mse': 0.69988, 'rmse': 0.83659}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 32, 'time_epoch': 0.67155, 'loss': 0.53773662, 'lr': 0, 'params': 13807345, 'time_iter': 0.04477, 'mae': 0.53774, 'r2': 0.72425, 'spearmanr': 0.85549, 'mse': 0.54553, 'rmse': 0.7386}\n",
            "> Epoch 32: took 16.4s (avg 16.3s) | Best so far: epoch 28\ttrain_loss: 0.3593 train_mae: 0.3593\tval_loss: 0.5929 val_mae: 0.5929\ttest_loss: 0.5410 test_mae: 0.5410\n",
            "train: {'epoch': 33, 'time_epoch': 14.4304, 'eta': 1617.34687, 'eta_hours': 0.44926, 'loss': 0.35052893, 'lr': 0.00016714, 'params': 13807345, 'time_iter': 0.14147, 'mae': 0.35053, 'r2': 0.87973, 'spearmanr': 0.94021, 'mse': 0.24708, 'rmse': 0.49708}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 33, 'time_epoch': 1.36127, 'loss': 0.60586675, 'lr': 0, 'params': 13807345, 'time_iter': 0.04694, 'mae': 0.60587, 'r2': 0.64974, 'spearmanr': 0.82167, 'mse': 0.7096, 'rmse': 0.84238}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 33, 'time_epoch': 0.71019, 'loss': 0.55317417, 'lr': 0, 'params': 13807345, 'time_iter': 0.04735, 'mae': 0.55317, 'r2': 0.71776, 'spearmanr': 0.85367, 'mse': 0.55837, 'rmse': 0.74724}\n",
            "> Epoch 33: took 16.5s (avg 16.3s) | Best so far: epoch 28\ttrain_loss: 0.3593 train_mae: 0.3593\tval_loss: 0.5929 val_mae: 0.5929\ttest_loss: 0.5410 test_mae: 0.5410\n",
            "train: {'epoch': 34, 'time_epoch': 14.45991, 'eta': 1605.10379, 'eta_hours': 0.44586, 'loss': 0.34809043, 'lr': 0.00016571, 'params': 13807345, 'time_iter': 0.14176, 'mae': 0.34809, 'r2': 0.88505, 'spearmanr': 0.94124, 'mse': 0.23615, 'rmse': 0.48596}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 34, 'time_epoch': 1.35179, 'loss': 0.60843924, 'lr': 0, 'params': 13807345, 'time_iter': 0.04661, 'mae': 0.60844, 'r2': 0.62784, 'spearmanr': 0.8143, 'mse': 0.75397, 'rmse': 0.86831}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 34, 'time_epoch': 0.72198, 'loss': 0.58032482, 'lr': 0, 'params': 13807345, 'time_iter': 0.04813, 'mae': 0.58032, 'r2': 0.66413, 'spearmanr': 0.82497, 'mse': 0.66447, 'rmse': 0.81515}\n",
            "> Epoch 34: took 16.6s (avg 16.3s) | Best so far: epoch 28\ttrain_loss: 0.3593 train_mae: 0.3593\tval_loss: 0.5929 val_mae: 0.5929\ttest_loss: 0.5410 test_mae: 0.5410\n",
            "train: {'epoch': 35, 'time_epoch': 13.57521, 'eta': 1589.93601, 'eta_hours': 0.44165, 'loss': 0.34495077, 'lr': 0.00016429, 'params': 13807345, 'time_iter': 0.13309, 'mae': 0.34495, 'r2': 0.88446, 'spearmanr': 0.94285, 'mse': 0.23736, 'rmse': 0.4872}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 35, 'time_epoch': 1.98799, 'loss': 0.59272881, 'lr': 0, 'params': 13807345, 'time_iter': 0.06855, 'mae': 0.59273, 'r2': 0.65748, 'spearmanr': 0.8298, 'mse': 0.69392, 'rmse': 0.83302}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 35, 'time_epoch': 0.90458, 'loss': 0.56444816, 'lr': 0, 'params': 13807345, 'time_iter': 0.06031, 'mae': 0.56445, 'r2': 0.71361, 'spearmanr': 0.85097, 'mse': 0.56658, 'rmse': 0.75271}\n",
            "> Epoch 35: took 16.5s (avg 16.3s) | Best so far: epoch 35\ttrain_loss: 0.3450 train_mae: 0.3449\tval_loss: 0.5927 val_mae: 0.5927\ttest_loss: 0.5644 test_mae: 0.5645\n",
            "train: {'epoch': 36, 'time_epoch': 12.47874, 'eta': 1571.50564, 'eta_hours': 0.43653, 'loss': 0.32657208, 'lr': 0.00016286, 'params': 13807345, 'time_iter': 0.12234, 'mae': 0.32657, 'r2': 0.89769, 'spearmanr': 0.94867, 'mse': 0.21018, 'rmse': 0.45846}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 36, 'time_epoch': 1.83929, 'loss': 0.59268756, 'lr': 0, 'params': 13807345, 'time_iter': 0.06342, 'mae': 0.59269, 'r2': 0.65607, 'spearmanr': 0.82907, 'mse': 0.69678, 'rmse': 0.83474}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 36, 'time_epoch': 1.01444, 'loss': 0.55419588, 'lr': 0, 'params': 13807345, 'time_iter': 0.06763, 'mae': 0.5542, 'r2': 0.70917, 'spearmanr': 0.84866, 'mse': 0.57536, 'rmse': 0.75852}\n",
            "> Epoch 36: took 15.4s (avg 16.3s) | Best so far: epoch 36\ttrain_loss: 0.3266 train_mae: 0.3266\tval_loss: 0.5927 val_mae: 0.5927\ttest_loss: 0.5542 test_mae: 0.5542\n",
            "train: {'epoch': 37, 'time_epoch': 12.8415, 'eta': 1554.45771, 'eta_hours': 0.43179, 'loss': 0.31540637, 'lr': 0.00016143, 'params': 13807345, 'time_iter': 0.1259, 'mae': 0.31541, 'r2': 0.90279, 'spearmanr': 0.9526, 'mse': 0.1997, 'rmse': 0.44688}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 37, 'time_epoch': 1.66737, 'loss': 0.57669883, 'lr': 0, 'params': 13807345, 'time_iter': 0.0575, 'mae': 0.5767, 'r2': 0.67618, 'spearmanr': 0.83781, 'mse': 0.65604, 'rmse': 0.80996}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 37, 'time_epoch': 0.91162, 'loss': 0.55910377, 'lr': 0, 'params': 13807345, 'time_iter': 0.06077, 'mae': 0.5591, 'r2': 0.71153, 'spearmanr': 0.85074, 'mse': 0.57069, 'rmse': 0.75544}\n",
            "> Epoch 37: took 15.5s (avg 16.3s) | Best so far: epoch 37\ttrain_loss: 0.3154 train_mae: 0.3154\tval_loss: 0.5767 val_mae: 0.5767\ttest_loss: 0.5591 test_mae: 0.5591\n",
            "train: {'epoch': 38, 'time_epoch': 13.70025, 'eta': 1540.06963, 'eta_hours': 0.4278, 'loss': 0.3193364, 'lr': 0.00016, 'params': 13807345, 'time_iter': 0.13432, 'mae': 0.31934, 'r2': 0.90079, 'spearmanr': 0.95021, 'mse': 0.20382, 'rmse': 0.45147}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 38, 'time_epoch': 1.29829, 'loss': 0.60668511, 'lr': 0, 'params': 13807345, 'time_iter': 0.04477, 'mae': 0.60669, 'r2': 0.64999, 'spearmanr': 0.82904, 'mse': 0.70909, 'rmse': 0.84207}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 38, 'time_epoch': 0.67048, 'loss': 0.55901696, 'lr': 0, 'params': 13807345, 'time_iter': 0.0447, 'mae': 0.55902, 'r2': 0.70857, 'spearmanr': 0.85262, 'mse': 0.57654, 'rmse': 0.7593}\n",
            "> Epoch 38: took 15.7s (avg 16.2s) | Best so far: epoch 37\ttrain_loss: 0.3154 train_mae: 0.3154\tval_loss: 0.5767 val_mae: 0.5767\ttest_loss: 0.5591 test_mae: 0.5591\n",
            "train: {'epoch': 39, 'time_epoch': 14.50907, 'eta': 1527.94018, 'eta_hours': 0.42443, 'loss': 0.33541378, 'lr': 0.00015857, 'params': 13807345, 'time_iter': 0.14225, 'mae': 0.33541, 'r2': 0.89401, 'spearmanr': 0.94596, 'mse': 0.21774, 'rmse': 0.46663}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 39, 'time_epoch': 1.29574, 'loss': 0.59101989, 'lr': 0, 'params': 13807345, 'time_iter': 0.04468, 'mae': 0.59102, 'r2': 0.66028, 'spearmanr': 0.83496, 'mse': 0.68824, 'rmse': 0.8296}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 39, 'time_epoch': 0.67984, 'loss': 0.54922456, 'lr': 0, 'params': 13807345, 'time_iter': 0.04532, 'mae': 0.54922, 'r2': 0.71659, 'spearmanr': 0.85143, 'mse': 0.56069, 'rmse': 0.74879}\n",
            "> Epoch 39: took 16.5s (avg 16.3s) | Best so far: epoch 37\ttrain_loss: 0.3154 train_mae: 0.3154\tval_loss: 0.5767 val_mae: 0.5767\ttest_loss: 0.5591 test_mae: 0.5591\n",
            "train: {'epoch': 40, 'time_epoch': 14.25367, 'eta': 1515.01567, 'eta_hours': 0.42084, 'loss': 0.30420133, 'lr': 0.00015714, 'params': 13807345, 'time_iter': 0.13974, 'mae': 0.3042, 'r2': 0.90782, 'spearmanr': 0.95455, 'mse': 0.18937, 'rmse': 0.43517}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 40, 'time_epoch': 1.32629, 'loss': 0.59284123, 'lr': 0, 'params': 13807345, 'time_iter': 0.04573, 'mae': 0.59284, 'r2': 0.65334, 'spearmanr': 0.83303, 'mse': 0.7023, 'rmse': 0.83803}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 40, 'time_epoch': 0.67515, 'loss': 0.5426306, 'lr': 0, 'params': 13807345, 'time_iter': 0.04501, 'mae': 0.54263, 'r2': 0.71675, 'spearmanr': 0.85359, 'mse': 0.56037, 'rmse': 0.74858}\n",
            "> Epoch 40: took 16.3s (avg 16.3s) | Best so far: epoch 37\ttrain_loss: 0.3154 train_mae: 0.3154\tval_loss: 0.5767 val_mae: 0.5767\ttest_loss: 0.5591 test_mae: 0.5591\n",
            "train: {'epoch': 41, 'time_epoch': 14.35598, 'eta': 1502.29096, 'eta_hours': 0.4173, 'loss': 0.31620407, 'lr': 0.00015571, 'params': 13807345, 'time_iter': 0.14074, 'mae': 0.3162, 'r2': 0.90482, 'spearmanr': 0.95252, 'mse': 0.19554, 'rmse': 0.4422}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 41, 'time_epoch': 1.35069, 'loss': 0.59239476, 'lr': 0, 'params': 13807345, 'time_iter': 0.04658, 'mae': 0.59239, 'r2': 0.65403, 'spearmanr': 0.82456, 'mse': 0.70091, 'rmse': 0.8372}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 41, 'time_epoch': 0.66938, 'loss': 0.55454805, 'lr': 0, 'params': 13807345, 'time_iter': 0.04463, 'mae': 0.55455, 'r2': 0.71341, 'spearmanr': 0.84898, 'mse': 0.56697, 'rmse': 0.75297}\n",
            "> Epoch 41: took 16.4s (avg 16.3s) | Best so far: epoch 37\ttrain_loss: 0.3154 train_mae: 0.3154\tval_loss: 0.5767 val_mae: 0.5767\ttest_loss: 0.5591 test_mae: 0.5591\n",
            "train: {'epoch': 42, 'time_epoch': 13.53672, 'eta': 1487.45174, 'eta_hours': 0.41318, 'loss': 0.30656349, 'lr': 0.00015429, 'params': 13807345, 'time_iter': 0.13271, 'mae': 0.30656, 'r2': 0.91014, 'spearmanr': 0.95538, 'mse': 0.18461, 'rmse': 0.42966}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 42, 'time_epoch': 1.96417, 'loss': 0.59864333, 'lr': 0, 'params': 13807345, 'time_iter': 0.06773, 'mae': 0.59864, 'r2': 0.64734, 'spearmanr': 0.82402, 'mse': 0.71447, 'rmse': 0.84526}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 42, 'time_epoch': 0.72435, 'loss': 0.55644657, 'lr': 0, 'params': 13807345, 'time_iter': 0.04829, 'mae': 0.55645, 'r2': 0.70817, 'spearmanr': 0.84805, 'mse': 0.57734, 'rmse': 0.75983}\n",
            "> Epoch 42: took 16.3s (avg 16.3s) | Best so far: epoch 37\ttrain_loss: 0.3154 train_mae: 0.3154\tval_loss: 0.5767 val_mae: 0.5767\ttest_loss: 0.5591 test_mae: 0.5591\n",
            "train: {'epoch': 43, 'time_epoch': 12.92204, 'eta': 1471.1909, 'eta_hours': 0.40866, 'loss': 0.31233187, 'lr': 0.00015286, 'params': 13807345, 'time_iter': 0.12669, 'mae': 0.31233, 'r2': 0.90726, 'spearmanr': 0.95383, 'mse': 0.19052, 'rmse': 0.43649}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 43, 'time_epoch': 1.97273, 'loss': 0.57858319, 'lr': 0, 'params': 13807345, 'time_iter': 0.06803, 'mae': 0.57858, 'r2': 0.67156, 'spearmanr': 0.83012, 'mse': 0.66539, 'rmse': 0.81572}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 43, 'time_epoch': 1.0661, 'loss': 0.52801152, 'lr': 0, 'params': 13807345, 'time_iter': 0.07107, 'mae': 0.52801, 'r2': 0.7305, 'spearmanr': 0.85523, 'mse': 0.53317, 'rmse': 0.73018}\n",
            "> Epoch 43: took 16.0s (avg 16.3s) | Best so far: epoch 37\ttrain_loss: 0.3154 train_mae: 0.3154\tval_loss: 0.5767 val_mae: 0.5767\ttest_loss: 0.5591 test_mae: 0.5591\n",
            "train: {'epoch': 44, 'time_epoch': 12.4902, 'eta': 1454.07083, 'eta_hours': 0.40391, 'loss': 0.30510662, 'lr': 0.00015143, 'params': 13807345, 'time_iter': 0.12245, 'mae': 0.30511, 'r2': 0.90646, 'spearmanr': 0.95299, 'mse': 0.19218, 'rmse': 0.43838}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 44, 'time_epoch': 1.75884, 'loss': 0.59305938, 'lr': 0, 'params': 13807345, 'time_iter': 0.06065, 'mae': 0.59306, 'r2': 0.65281, 'spearmanr': 0.82675, 'mse': 0.70339, 'rmse': 0.83868}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 44, 'time_epoch': 0.95937, 'loss': 0.54976089, 'lr': 0, 'params': 13807345, 'time_iter': 0.06396, 'mae': 0.54976, 'r2': 0.70993, 'spearmanr': 0.84874, 'mse': 0.57386, 'rmse': 0.75754}\n",
            "> Epoch 44: took 15.3s (avg 16.2s) | Best so far: epoch 37\ttrain_loss: 0.3154 train_mae: 0.3154\tval_loss: 0.5767 val_mae: 0.5767\ttest_loss: 0.5591 test_mae: 0.5591\n",
            "train: {'epoch': 45, 'time_epoch': 13.63779, 'eta': 1439.74663, 'eta_hours': 0.39993, 'loss': 0.29750856, 'lr': 0.00015, 'params': 13807345, 'time_iter': 0.1337, 'mae': 0.29751, 'r2': 0.9162, 'spearmanr': 0.95922, 'mse': 0.17217, 'rmse': 0.41493}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 45, 'time_epoch': 1.3348, 'loss': 0.58671688, 'lr': 0, 'params': 13807345, 'time_iter': 0.04603, 'mae': 0.58672, 'r2': 0.6691, 'spearmanr': 0.83013, 'mse': 0.67038, 'rmse': 0.81876}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 45, 'time_epoch': 0.69727, 'loss': 0.55440749, 'lr': 0, 'params': 13807345, 'time_iter': 0.04648, 'mae': 0.55441, 'r2': 0.7096, 'spearmanr': 0.84464, 'mse': 0.57451, 'rmse': 0.75797}\n",
            "> Epoch 45: took 15.7s (avg 16.2s) | Best so far: epoch 37\ttrain_loss: 0.3154 train_mae: 0.3154\tval_loss: 0.5767 val_mae: 0.5767\ttest_loss: 0.5591 test_mae: 0.5591\n",
            "train: {'epoch': 46, 'time_epoch': 14.35945, 'eta': 1427.03314, 'eta_hours': 0.3964, 'loss': 0.29366185, 'lr': 0.00014857, 'params': 13807345, 'time_iter': 0.14078, 'mae': 0.29366, 'r2': 0.9169, 'spearmanr': 0.95876, 'mse': 0.17071, 'rmse': 0.41318}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 46, 'time_epoch': 1.31888, 'loss': 0.58320012, 'lr': 0, 'params': 13807345, 'time_iter': 0.04548, 'mae': 0.5832, 'r2': 0.66362, 'spearmanr': 0.8314, 'mse': 0.68148, 'rmse': 0.82552}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 46, 'time_epoch': 0.68762, 'loss': 0.53381374, 'lr': 0, 'params': 13807345, 'time_iter': 0.04584, 'mae': 0.53381, 'r2': 0.72187, 'spearmanr': 0.85285, 'mse': 0.55023, 'rmse': 0.74178}\n",
            "> Epoch 46: took 16.4s (avg 16.2s) | Best so far: epoch 37\ttrain_loss: 0.3154 train_mae: 0.3154\tval_loss: 0.5767 val_mae: 0.5767\ttest_loss: 0.5591 test_mae: 0.5591\n",
            "train: {'epoch': 47, 'time_epoch': 14.425, 'eta': 1414.39036, 'eta_hours': 0.39289, 'loss': 0.29230551, 'lr': 0.00014714, 'params': 13807345, 'time_iter': 0.14142, 'mae': 0.29231, 'r2': 0.91823, 'spearmanr': 0.95924, 'mse': 0.16799, 'rmse': 0.40987}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 47, 'time_epoch': 1.3156, 'loss': 0.58479754, 'lr': 0, 'params': 13807345, 'time_iter': 0.04537, 'mae': 0.5848, 'r2': 0.66624, 'spearmanr': 0.83048, 'mse': 0.67617, 'rmse': 0.8223}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 47, 'time_epoch': 0.69017, 'loss': 0.53619763, 'lr': 0, 'params': 13807345, 'time_iter': 0.04601, 'mae': 0.5362, 'r2': 0.72025, 'spearmanr': 0.85251, 'mse': 0.55344, 'rmse': 0.74393}\n",
            "> Epoch 47: took 16.5s (avg 16.2s) | Best so far: epoch 37\ttrain_loss: 0.3154 train_mae: 0.3154\tval_loss: 0.5767 val_mae: 0.5767\ttest_loss: 0.5591 test_mae: 0.5591\n",
            "train: {'epoch': 48, 'time_epoch': 14.48024, 'eta': 1401.78869, 'eta_hours': 0.38939, 'loss': 0.2871618, 'lr': 0.00014571, 'params': 13807345, 'time_iter': 0.14196, 'mae': 0.28716, 'r2': 0.9184, 'spearmanr': 0.95935, 'mse': 0.16763, 'rmse': 0.40943}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 48, 'time_epoch': 1.34744, 'loss': 0.58222772, 'lr': 0, 'params': 13807345, 'time_iter': 0.04646, 'mae': 0.58223, 'r2': 0.66978, 'spearmanr': 0.83527, 'mse': 0.669, 'rmse': 0.81792}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 48, 'time_epoch': 0.67662, 'loss': 0.53735032, 'lr': 0, 'params': 13807345, 'time_iter': 0.04511, 'mae': 0.53735, 'r2': 0.7209, 'spearmanr': 0.856, 'mse': 0.55215, 'rmse': 0.74307}\n",
            "> Epoch 48: took 16.5s (avg 16.2s) | Best so far: epoch 37\ttrain_loss: 0.3154 train_mae: 0.3154\tval_loss: 0.5767 val_mae: 0.5767\ttest_loss: 0.5591 test_mae: 0.5591\n",
            "train: {'epoch': 49, 'time_epoch': 14.18809, 'eta': 1388.52758, 'eta_hours': 0.3857, 'loss': 0.28116908, 'lr': 0.00014429, 'params': 13807345, 'time_iter': 0.1391, 'mae': 0.28117, 'r2': 0.92367, 'spearmanr': 0.96302, 'mse': 0.15681, 'rmse': 0.39599}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 49, 'time_epoch': 1.59898, 'loss': 0.57669681, 'lr': 0, 'params': 13807345, 'time_iter': 0.05514, 'mae': 0.5767, 'r2': 0.67026, 'spearmanr': 0.83258, 'mse': 0.66802, 'rmse': 0.81732}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 49, 'time_epoch': 0.69327, 'loss': 0.53132503, 'lr': 0, 'params': 13807345, 'time_iter': 0.04622, 'mae': 0.53133, 'r2': 0.72715, 'spearmanr': 0.8552, 'mse': 0.53979, 'rmse': 0.7347}\n",
            "> Epoch 49: took 16.5s (avg 16.2s) | Best so far: epoch 37\ttrain_loss: 0.3154 train_mae: 0.3154\tval_loss: 0.5767 val_mae: 0.5767\ttest_loss: 0.5591 test_mae: 0.5591\n",
            "train: {'epoch': 50, 'time_epoch': 13.19662, 'eta': 1373.30551, 'eta_hours': 0.38147, 'loss': 0.28110795, 'lr': 0.00014286, 'params': 13807345, 'time_iter': 0.12938, 'mae': 0.28111, 'r2': 0.92277, 'spearmanr': 0.96093, 'mse': 0.15866, 'rmse': 0.39832}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 50, 'time_epoch': 2.11477, 'loss': 0.58647748, 'lr': 0, 'params': 13807345, 'time_iter': 0.07292, 'mae': 0.58648, 'r2': 0.65704, 'spearmanr': 0.82736, 'mse': 0.6948, 'rmse': 0.83355}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 50, 'time_epoch': 1.05981, 'loss': 0.53808472, 'lr': 0, 'params': 13807345, 'time_iter': 0.07065, 'mae': 0.53808, 'r2': 0.71399, 'spearmanr': 0.84563, 'mse': 0.56582, 'rmse': 0.75221}\n",
            "> Epoch 50: took 16.4s (avg 16.2s) | Best so far: epoch 37\ttrain_loss: 0.3154 train_mae: 0.3154\tval_loss: 0.5767 val_mae: 0.5767\ttest_loss: 0.5591 test_mae: 0.5591\n",
            "train: {'epoch': 51, 'time_epoch': 12.48428, 'eta': 1356.81884, 'eta_hours': 0.37689, 'loss': 0.27264743, 'lr': 0.00014143, 'params': 13807345, 'time_iter': 0.12239, 'mae': 0.27265, 'r2': 0.92878, 'spearmanr': 0.96441, 'mse': 0.14631, 'rmse': 0.38251}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 51, 'time_epoch': 1.7382, 'loss': 0.58126248, 'lr': 0, 'params': 13807345, 'time_iter': 0.05994, 'mae': 0.58126, 'r2': 0.66275, 'spearmanr': 0.82888, 'mse': 0.68324, 'rmse': 0.82658}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 51, 'time_epoch': 0.97619, 'loss': 0.5406004, 'lr': 0, 'params': 13807345, 'time_iter': 0.06508, 'mae': 0.5406, 'r2': 0.7163, 'spearmanr': 0.85013, 'mse': 0.56126, 'rmse': 0.74917}\n",
            "> Epoch 51: took 15.2s (avg 16.2s) | Best so far: epoch 37\ttrain_loss: 0.3154 train_mae: 0.3154\tval_loss: 0.5767 val_mae: 0.5767\ttest_loss: 0.5591 test_mae: 0.5591\n",
            "train: {'epoch': 52, 'time_epoch': 13.56146, 'eta': 1342.45466, 'eta_hours': 0.3729, 'loss': 0.27648374, 'lr': 0.00014, 'params': 13807345, 'time_iter': 0.13296, 'mae': 0.27648, 'r2': 0.92656, 'spearmanr': 0.96278, 'mse': 0.15088, 'rmse': 0.38844}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 52, 'time_epoch': 1.43364, 'loss': 0.59664881, 'lr': 0, 'params': 13807345, 'time_iter': 0.04944, 'mae': 0.59665, 'r2': 0.64888, 'spearmanr': 0.82119, 'mse': 0.71135, 'rmse': 0.84342}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 52, 'time_epoch': 0.88429, 'loss': 0.54522956, 'lr': 0, 'params': 13807345, 'time_iter': 0.05895, 'mae': 0.54523, 'r2': 0.71483, 'spearmanr': 0.85014, 'mse': 0.56416, 'rmse': 0.75111}\n",
            "> Epoch 52: took 15.9s (avg 16.2s) | Best so far: epoch 37\ttrain_loss: 0.3154 train_mae: 0.3154\tval_loss: 0.5767 val_mae: 0.5767\ttest_loss: 0.5591 test_mae: 0.5591\n",
            "train: {'epoch': 53, 'time_epoch': 13.90465, 'eta': 1328.73032, 'eta_hours': 0.36909, 'loss': 0.27552888, 'lr': 0.00013857, 'params': 13807345, 'time_iter': 0.13632, 'mae': 0.27553, 'r2': 0.92638, 'spearmanr': 0.96357, 'mse': 0.15125, 'rmse': 0.3889}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 53, 'time_epoch': 1.33443, 'loss': 0.5858565, 'lr': 0, 'params': 13807345, 'time_iter': 0.04601, 'mae': 0.58586, 'r2': 0.66982, 'spearmanr': 0.82983, 'mse': 0.66891, 'rmse': 0.81787}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 53, 'time_epoch': 0.66795, 'loss': 0.55387087, 'lr': 0, 'params': 13807345, 'time_iter': 0.04453, 'mae': 0.55387, 'r2': 0.70477, 'spearmanr': 0.84332, 'mse': 0.58407, 'rmse': 0.76424}\n",
            "> Epoch 53: took 15.9s (avg 16.2s) | Best so far: epoch 37\ttrain_loss: 0.3154 train_mae: 0.3154\tval_loss: 0.5767 val_mae: 0.5767\ttest_loss: 0.5591 test_mae: 0.5591\n",
            "train: {'epoch': 54, 'time_epoch': 14.35348, 'eta': 1315.77468, 'eta_hours': 0.36549, 'loss': 0.2658075, 'lr': 0.00013714, 'params': 13807345, 'time_iter': 0.14072, 'mae': 0.26581, 'r2': 0.93009, 'spearmanr': 0.96543, 'mse': 0.14363, 'rmse': 0.37899}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 54, 'time_epoch': 1.31226, 'loss': 0.58538115, 'lr': 0, 'params': 13807345, 'time_iter': 0.04525, 'mae': 0.58538, 'r2': 0.66628, 'spearmanr': 0.8291, 'mse': 0.67608, 'rmse': 0.82224}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 54, 'time_epoch': 0.67059, 'loss': 0.5289145, 'lr': 0, 'params': 13807345, 'time_iter': 0.04471, 'mae': 0.52891, 'r2': 0.72431, 'spearmanr': 0.85485, 'mse': 0.54541, 'rmse': 0.73852}\n",
            "> Epoch 54: took 16.4s (avg 16.2s) | Best so far: epoch 37\ttrain_loss: 0.3154 train_mae: 0.3154\tval_loss: 0.5767 val_mae: 0.5767\ttest_loss: 0.5591 test_mae: 0.5591\n",
            "train: {'epoch': 55, 'time_epoch': 14.32772, 'eta': 1302.72586, 'eta_hours': 0.36187, 'loss': 0.2726422, 'lr': 0.00013571, 'params': 13807345, 'time_iter': 0.14047, 'mae': 0.27264, 'r2': 0.92863, 'spearmanr': 0.96403, 'mse': 0.14662, 'rmse': 0.38291}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 55, 'time_epoch': 1.31798, 'loss': 0.58673908, 'lr': 0, 'params': 13807345, 'time_iter': 0.04545, 'mae': 0.58674, 'r2': 0.65272, 'spearmanr': 0.82967, 'mse': 0.70357, 'rmse': 0.83879}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 55, 'time_epoch': 0.6844, 'loss': 0.54757365, 'lr': 0, 'params': 13807345, 'time_iter': 0.04563, 'mae': 0.54757, 'r2': 0.70649, 'spearmanr': 0.8525, 'mse': 0.58067, 'rmse': 0.76202}\n",
            "> Epoch 55: took 16.4s (avg 16.2s) | Best so far: epoch 37\ttrain_loss: 0.3154 train_mae: 0.3154\tval_loss: 0.5767 val_mae: 0.5767\ttest_loss: 0.5591 test_mae: 0.5591\n",
            "train: {'epoch': 56, 'time_epoch': 14.61525, 'eta': 1290.10131, 'eta_hours': 0.35836, 'loss': 0.26000255, 'lr': 0.00013429, 'params': 13807345, 'time_iter': 0.14329, 'mae': 0.26, 'r2': 0.93146, 'spearmanr': 0.96583, 'mse': 0.14081, 'rmse': 0.37524}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 56, 'time_epoch': 1.3176, 'loss': 0.57528366, 'lr': 0, 'params': 13807345, 'time_iter': 0.04543, 'mae': 0.57528, 'r2': 0.67742, 'spearmanr': 0.83183, 'mse': 0.65353, 'rmse': 0.80841}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 56, 'time_epoch': 0.69048, 'loss': 0.53884413, 'lr': 0, 'params': 13807345, 'time_iter': 0.04603, 'mae': 0.53884, 'r2': 0.71397, 'spearmanr': 0.84825, 'mse': 0.56587, 'rmse': 0.75224}\n",
            "> Epoch 56: took 16.7s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 57, 'time_epoch': 13.49554, 'eta': 1275.63202, 'eta_hours': 0.35434, 'loss': 0.26189516, 'lr': 0.00013286, 'params': 13807345, 'time_iter': 0.13231, 'mae': 0.2619, 'r2': 0.93496, 'spearmanr': 0.96747, 'mse': 0.13363, 'rmse': 0.36555}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 57, 'time_epoch': 1.9968, 'loss': 0.58050755, 'lr': 0, 'params': 13807345, 'time_iter': 0.06886, 'mae': 0.58051, 'r2': 0.67681, 'spearmanr': 0.83226, 'mse': 0.65475, 'rmse': 0.80917}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 57, 'time_epoch': 0.9033, 'loss': 0.53533377, 'lr': 0, 'params': 13807345, 'time_iter': 0.06022, 'mae': 0.53533, 'r2': 0.71822, 'spearmanr': 0.84851, 'mse': 0.55747, 'rmse': 0.74664}\n",
            "> Epoch 57: took 16.4s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 58, 'time_epoch': 12.40341, 'eta': 1259.51126, 'eta_hours': 0.34986, 'loss': 0.25897646, 'lr': 0.00013143, 'params': 13807345, 'time_iter': 0.1216, 'mae': 0.25898, 'r2': 0.93309, 'spearmanr': 0.96699, 'mse': 0.13745, 'rmse': 0.37075}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 58, 'time_epoch': 1.88139, 'loss': 0.58793444, 'lr': 0, 'params': 13807345, 'time_iter': 0.06488, 'mae': 0.58793, 'r2': 0.66522, 'spearmanr': 0.82867, 'mse': 0.67824, 'rmse': 0.82356}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 58, 'time_epoch': 1.03169, 'loss': 0.55287508, 'lr': 0, 'params': 13807345, 'time_iter': 0.06878, 'mae': 0.55288, 'r2': 0.70412, 'spearmanr': 0.8449, 'mse': 0.58536, 'rmse': 0.76509}\n",
            "> Epoch 58: took 15.4s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 59, 'time_epoch': 12.76491, 'eta': 1244.05666, 'eta_hours': 0.34557, 'loss': 0.26031421, 'lr': 0.00013, 'params': 13807345, 'time_iter': 0.12515, 'mae': 0.26031, 'r2': 0.93305, 'spearmanr': 0.96719, 'mse': 0.13755, 'rmse': 0.37088}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 59, 'time_epoch': 1.67569, 'loss': 0.58657211, 'lr': 0, 'params': 13807345, 'time_iter': 0.05778, 'mae': 0.58657, 'r2': 0.66643, 'spearmanr': 0.82731, 'mse': 0.67578, 'rmse': 0.82206}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 59, 'time_epoch': 0.9281, 'loss': 0.54242405, 'lr': 0, 'params': 13807345, 'time_iter': 0.06187, 'mae': 0.54242, 'r2': 0.70816, 'spearmanr': 0.85123, 'mse': 0.57736, 'rmse': 0.75984}\n",
            "> Epoch 59: took 15.4s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 60, 'time_epoch': 14.16461, 'eta': 1230.73244, 'eta_hours': 0.34187, 'loss': 0.25953423, 'lr': 0.00012857, 'params': 13807345, 'time_iter': 0.13887, 'mae': 0.25953, 'r2': 0.93546, 'spearmanr': 0.96715, 'mse': 0.13259, 'rmse': 0.36413}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 60, 'time_epoch': 1.34676, 'loss': 0.58850165, 'lr': 0, 'params': 13807345, 'time_iter': 0.04644, 'mae': 0.5885, 'r2': 0.65391, 'spearmanr': 0.81916, 'mse': 0.70116, 'rmse': 0.83735}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 60, 'time_epoch': 0.71397, 'loss': 0.54630367, 'lr': 0, 'params': 13807345, 'time_iter': 0.0476, 'mae': 0.5463, 'r2': 0.69811, 'spearmanr': 0.84095, 'mse': 0.59723, 'rmse': 0.77281}\n",
            "> Epoch 60: took 16.3s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 61, 'time_epoch': 14.57067, 'eta': 1217.95746, 'eta_hours': 0.33832, 'loss': 0.24845068, 'lr': 0.00012714, 'params': 13807345, 'time_iter': 0.14285, 'mae': 0.24845, 'r2': 0.93956, 'spearmanr': 0.96952, 'mse': 0.12418, 'rmse': 0.35239}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 61, 'time_epoch': 1.31821, 'loss': 0.58661188, 'lr': 0, 'params': 13807345, 'time_iter': 0.04546, 'mae': 0.58661, 'r2': 0.66402, 'spearmanr': 0.82793, 'mse': 0.68068, 'rmse': 0.82503}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 61, 'time_epoch': 0.70813, 'loss': 0.54597697, 'lr': 0, 'params': 13807345, 'time_iter': 0.04721, 'mae': 0.54598, 'r2': 0.71735, 'spearmanr': 0.85417, 'mse': 0.55918, 'rmse': 0.74778}\n",
            "> Epoch 61: took 16.6s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 62, 'time_epoch': 14.48164, 'eta': 1205.00252, 'eta_hours': 0.33472, 'loss': 0.25165165, 'lr': 0.00012571, 'params': 13807345, 'time_iter': 0.14198, 'mae': 0.25165, 'r2': 0.93805, 'spearmanr': 0.96928, 'mse': 0.12727, 'rmse': 0.35676}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 62, 'time_epoch': 1.34974, 'loss': 0.58399366, 'lr': 0, 'params': 13807345, 'time_iter': 0.04654, 'mae': 0.58399, 'r2': 0.66162, 'spearmanr': 0.82233, 'mse': 0.68554, 'rmse': 0.82797}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 62, 'time_epoch': 0.69308, 'loss': 0.52564242, 'lr': 0, 'params': 13807345, 'time_iter': 0.04621, 'mae': 0.52564, 'r2': 0.72084, 'spearmanr': 0.85308, 'mse': 0.55227, 'rmse': 0.74315}\n",
            "> Epoch 62: took 16.6s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 63, 'time_epoch': 14.29978, 'eta': 1191.75549, 'eta_hours': 0.33104, 'loss': 0.24325639, 'lr': 0.00012429, 'params': 13807345, 'time_iter': 0.14019, 'mae': 0.24326, 'r2': 0.94185, 'spearmanr': 0.97152, 'mse': 0.11947, 'rmse': 0.34564}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 63, 'time_epoch': 1.30766, 'loss': 0.58637858, 'lr': 0, 'params': 13807345, 'time_iter': 0.04509, 'mae': 0.58638, 'r2': 0.65538, 'spearmanr': 0.82476, 'mse': 0.69817, 'rmse': 0.83556}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 63, 'time_epoch': 0.68889, 'loss': 0.5486192, 'lr': 0, 'params': 13807345, 'time_iter': 0.04593, 'mae': 0.54862, 'r2': 0.70587, 'spearmanr': 0.85005, 'mse': 0.5819, 'rmse': 0.76282}\n",
            "> Epoch 63: took 16.3s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 64, 'time_epoch': 14.24577, 'eta': 1178.40545, 'eta_hours': 0.32733, 'loss': 0.24624317, 'lr': 0.00012286, 'params': 13807345, 'time_iter': 0.13966, 'mae': 0.24624, 'r2': 0.94086, 'spearmanr': 0.97105, 'mse': 0.12149, 'rmse': 0.34856}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 64, 'time_epoch': 1.53081, 'loss': 0.59156717, 'lr': 0, 'params': 13807345, 'time_iter': 0.05279, 'mae': 0.59157, 'r2': 0.66125, 'spearmanr': 0.82492, 'mse': 0.68628, 'rmse': 0.82842}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 64, 'time_epoch': 0.68239, 'loss': 0.55683792, 'lr': 0, 'params': 13807345, 'time_iter': 0.04549, 'mae': 0.55684, 'r2': 0.69206, 'spearmanr': 0.83942, 'mse': 0.6092, 'rmse': 0.78052}\n",
            "> Epoch 64: took 16.5s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 65, 'time_epoch': 13.02472, 'eta': 1163.4742, 'eta_hours': 0.32319, 'loss': 0.23633149, 'lr': 0.00012143, 'params': 13807345, 'time_iter': 0.12769, 'mae': 0.23633, 'r2': 0.94518, 'spearmanr': 0.97225, 'mse': 0.11261, 'rmse': 0.33558}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 65, 'time_epoch': 2.05994, 'loss': 0.58414388, 'lr': 0, 'params': 13807345, 'time_iter': 0.07103, 'mae': 0.58414, 'r2': 0.66479, 'spearmanr': 0.83239, 'mse': 0.67911, 'rmse': 0.82408}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 65, 'time_epoch': 1.03743, 'loss': 0.55010313, 'lr': 0, 'params': 13807345, 'time_iter': 0.06916, 'mae': 0.5501, 'r2': 0.7015, 'spearmanr': 0.84622, 'mse': 0.59054, 'rmse': 0.76847}\n",
            "> Epoch 65: took 16.2s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 66, 'time_epoch': 12.45669, 'eta': 1147.89618, 'eta_hours': 0.31886, 'loss': 0.24061575, 'lr': 0.00012, 'params': 13807345, 'time_iter': 0.12212, 'mae': 0.24062, 'r2': 0.94334, 'spearmanr': 0.97205, 'mse': 0.11641, 'rmse': 0.34119}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 66, 'time_epoch': 1.77961, 'loss': 0.57628814, 'lr': 0, 'params': 13807345, 'time_iter': 0.06137, 'mae': 0.57629, 'r2': 0.6799, 'spearmanr': 0.83375, 'mse': 0.6485, 'rmse': 0.80529}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 66, 'time_epoch': 1.00074, 'loss': 0.54457342, 'lr': 0, 'params': 13807345, 'time_iter': 0.06672, 'mae': 0.54457, 'r2': 0.70642, 'spearmanr': 0.84596, 'mse': 0.58081, 'rmse': 0.76211}\n",
            "> Epoch 66: took 15.3s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 67, 'time_epoch': 13.12672, 'eta': 1133.21793, 'eta_hours': 0.31478, 'loss': 0.23620672, 'lr': 0.00011857, 'params': 13807345, 'time_iter': 0.12869, 'mae': 0.23621, 'r2': 0.94577, 'spearmanr': 0.97371, 'mse': 0.11141, 'rmse': 0.33377}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 67, 'time_epoch': 1.35404, 'loss': 0.59674841, 'lr': 0, 'params': 13807345, 'time_iter': 0.04669, 'mae': 0.59675, 'r2': 0.64998, 'spearmanr': 0.82328, 'mse': 0.70911, 'rmse': 0.84209}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 67, 'time_epoch': 0.90997, 'loss': 0.55183148, 'lr': 0, 'params': 13807345, 'time_iter': 0.06066, 'mae': 0.55183, 'r2': 0.70646, 'spearmanr': 0.84525, 'mse': 0.58072, 'rmse': 0.76205}\n",
            "> Epoch 67: took 15.4s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 68, 'time_epoch': 14.13212, 'eta': 1119.76492, 'eta_hours': 0.31105, 'loss': 0.23639319, 'lr': 0.00011714, 'params': 13807345, 'time_iter': 0.13855, 'mae': 0.23639, 'r2': 0.94614, 'spearmanr': 0.97347, 'mse': 0.11065, 'rmse': 0.33265}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 68, 'time_epoch': 1.32219, 'loss': 0.57745338, 'lr': 0, 'params': 13807345, 'time_iter': 0.04559, 'mae': 0.57745, 'r2': 0.6745, 'spearmanr': 0.82835, 'mse': 0.65944, 'rmse': 0.81206}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 68, 'time_epoch': 0.70582, 'loss': 0.54312779, 'lr': 0, 'params': 13807345, 'time_iter': 0.04705, 'mae': 0.54313, 'r2': 0.70925, 'spearmanr': 0.84273, 'mse': 0.57521, 'rmse': 0.75842}\n",
            "> Epoch 68: took 16.2s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 69, 'time_epoch': 14.53403, 'eta': 1106.75184, 'eta_hours': 0.30743, 'loss': 0.23427296, 'lr': 0.00011571, 'params': 13807345, 'time_iter': 0.14249, 'mae': 0.23427, 'r2': 0.94375, 'spearmanr': 0.97175, 'mse': 0.11556, 'rmse': 0.33994}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 69, 'time_epoch': 1.31884, 'loss': 0.58882859, 'lr': 0, 'params': 13807345, 'time_iter': 0.04548, 'mae': 0.58883, 'r2': 0.6668, 'spearmanr': 0.82544, 'mse': 0.67503, 'rmse': 0.8216}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 69, 'time_epoch': 0.69321, 'loss': 0.55328226, 'lr': 0, 'params': 13807345, 'time_iter': 0.04621, 'mae': 0.55328, 'r2': 0.69988, 'spearmanr': 0.83843, 'mse': 0.59375, 'rmse': 0.77055}\n",
            "> Epoch 69: took 16.6s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 70, 'time_epoch': 14.38746, 'eta': 1093.53281, 'eta_hours': 0.30376, 'loss': 0.23313612, 'lr': 0.00011429, 'params': 13807345, 'time_iter': 0.14105, 'mae': 0.23314, 'r2': 0.94595, 'spearmanr': 0.9727, 'mse': 0.11104, 'rmse': 0.33322}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 70, 'time_epoch': 1.31115, 'loss': 0.58559996, 'lr': 0, 'params': 13807345, 'time_iter': 0.04521, 'mae': 0.5856, 'r2': 0.6633, 'spearmanr': 0.82746, 'mse': 0.68213, 'rmse': 0.82591}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 70, 'time_epoch': 0.68468, 'loss': 0.54083285, 'lr': 0, 'params': 13807345, 'time_iter': 0.04565, 'mae': 0.54083, 'r2': 0.70973, 'spearmanr': 0.85025, 'mse': 0.57426, 'rmse': 0.7578}\n",
            "> Epoch 70: took 16.4s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 71, 'time_epoch': 14.40119, 'eta': 1080.29621, 'eta_hours': 0.30008, 'loss': 0.23276461, 'lr': 0.00011286, 'params': 13807345, 'time_iter': 0.14119, 'mae': 0.23276, 'r2': 0.94719, 'spearmanr': 0.9737, 'mse': 0.10849, 'rmse': 0.32938}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 71, 'time_epoch': 1.32375, 'loss': 0.5815965, 'lr': 0, 'params': 13807345, 'time_iter': 0.04565, 'mae': 0.5816, 'r2': 0.67202, 'spearmanr': 0.82895, 'mse': 0.66446, 'rmse': 0.81515}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 71, 'time_epoch': 0.69024, 'loss': 0.53785053, 'lr': 0, 'params': 13807345, 'time_iter': 0.04602, 'mae': 0.53785, 'r2': 0.71486, 'spearmanr': 0.8466, 'mse': 0.56411, 'rmse': 0.75108}\n",
            "> Epoch 71: took 16.5s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 72, 'time_epoch': 13.30829, 'eta': 1065.87492, 'eta_hours': 0.29608, 'loss': 0.2303017, 'lr': 0.00011143, 'params': 13807345, 'time_iter': 0.13047, 'mae': 0.2303, 'r2': 0.94778, 'spearmanr': 0.97398, 'mse': 0.10727, 'rmse': 0.32753}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 72, 'time_epoch': 1.9555, 'loss': 0.59491159, 'lr': 0, 'params': 13807345, 'time_iter': 0.06743, 'mae': 0.59491, 'r2': 0.65176, 'spearmanr': 0.83063, 'mse': 0.70551, 'rmse': 0.83994}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 72, 'time_epoch': 0.95357, 'loss': 0.5537201, 'lr': 0, 'params': 13807345, 'time_iter': 0.06357, 'mae': 0.55372, 'r2': 0.69502, 'spearmanr': 0.84639, 'mse': 0.60335, 'rmse': 0.77676}\n",
            "> Epoch 72: took 16.3s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 73, 'time_epoch': 12.60901, 'eta': 1050.76552, 'eta_hours': 0.29188, 'loss': 0.21707944, 'lr': 0.00011, 'params': 13807345, 'time_iter': 0.12362, 'mae': 0.21708, 'r2': 0.95366, 'spearmanr': 0.97719, 'mse': 0.0952, 'rmse': 0.30854}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 73, 'time_epoch': 1.89588, 'loss': 0.59987095, 'lr': 0, 'params': 13807345, 'time_iter': 0.06538, 'mae': 0.59987, 'r2': 0.65514, 'spearmanr': 0.82851, 'mse': 0.69867, 'rmse': 0.83586}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 73, 'time_epoch': 1.03892, 'loss': 0.56536748, 'lr': 0, 'params': 13807345, 'time_iter': 0.06926, 'mae': 0.56537, 'r2': 0.69372, 'spearmanr': 0.84342, 'mse': 0.60592, 'rmse': 0.77841}\n",
            "> Epoch 73: took 15.6s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 74, 'time_epoch': 12.67794, 'eta': 1035.79174, 'eta_hours': 0.28772, 'loss': 0.21827488, 'lr': 0.00010857, 'params': 13807345, 'time_iter': 0.12429, 'mae': 0.21827, 'r2': 0.95368, 'spearmanr': 0.97728, 'mse': 0.09516, 'rmse': 0.30847}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 74, 'time_epoch': 1.60522, 'loss': 0.58852334, 'lr': 0, 'params': 13807345, 'time_iter': 0.05535, 'mae': 0.58852, 'r2': 0.66575, 'spearmanr': 0.82666, 'mse': 0.67717, 'rmse': 0.8229}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 74, 'time_epoch': 0.8956, 'loss': 0.53364872, 'lr': 0, 'params': 13807345, 'time_iter': 0.05971, 'mae': 0.53365, 'r2': 0.71703, 'spearmanr': 0.84849, 'mse': 0.55982, 'rmse': 0.74821}\n",
            "> Epoch 74: took 15.2s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 75, 'time_epoch': 13.8819, 'eta': 1022.05065, 'eta_hours': 0.2839, 'loss': 0.21923226, 'lr': 0.00010714, 'params': 13807345, 'time_iter': 0.1361, 'mae': 0.21923, 'r2': 0.95163, 'spearmanr': 0.97541, 'mse': 0.09937, 'rmse': 0.31522}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 75, 'time_epoch': 1.31932, 'loss': 0.59071478, 'lr': 0, 'params': 13807345, 'time_iter': 0.04549, 'mae': 0.59071, 'r2': 0.66013, 'spearmanr': 0.82729, 'mse': 0.68855, 'rmse': 0.82979}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 75, 'time_epoch': 0.70052, 'loss': 0.54262993, 'lr': 0, 'params': 13807345, 'time_iter': 0.0467, 'mae': 0.54263, 'r2': 0.71002, 'spearmanr': 0.84828, 'mse': 0.57368, 'rmse': 0.75742}\n",
            "> Epoch 75: took 15.9s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 76, 'time_epoch': 14.27544, 'eta': 1008.679, 'eta_hours': 0.28019, 'loss': 0.21965864, 'lr': 0.00010571, 'params': 13807345, 'time_iter': 0.13996, 'mae': 0.21966, 'r2': 0.95192, 'spearmanr': 0.97606, 'mse': 0.09878, 'rmse': 0.31429}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 76, 'time_epoch': 1.35244, 'loss': 0.58372568, 'lr': 0, 'params': 13807345, 'time_iter': 0.04664, 'mae': 0.58373, 'r2': 0.66715, 'spearmanr': 0.82934, 'mse': 0.67433, 'rmse': 0.82118}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 76, 'time_epoch': 0.68978, 'loss': 0.54752943, 'lr': 0, 'params': 13807345, 'time_iter': 0.04599, 'mae': 0.54753, 'r2': 0.70325, 'spearmanr': 0.84388, 'mse': 0.58708, 'rmse': 0.76621}\n",
            "> Epoch 76: took 16.4s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 77, 'time_epoch': 14.5244, 'eta': 995.51398, 'eta_hours': 0.27653, 'loss': 0.21360889, 'lr': 0.00010429, 'params': 13807345, 'time_iter': 0.1424, 'mae': 0.21361, 'r2': 0.95442, 'spearmanr': 0.97732, 'mse': 0.09364, 'rmse': 0.30601}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 77, 'time_epoch': 1.30568, 'loss': 0.58032322, 'lr': 0, 'params': 13807345, 'time_iter': 0.04502, 'mae': 0.58032, 'r2': 0.67193, 'spearmanr': 0.82835, 'mse': 0.66464, 'rmse': 0.81526}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 77, 'time_epoch': 0.68262, 'loss': 0.53825802, 'lr': 0, 'params': 13807345, 'time_iter': 0.04551, 'mae': 0.53826, 'r2': 0.71183, 'spearmanr': 0.84575, 'mse': 0.57009, 'rmse': 0.75505}\n",
            "> Epoch 77: took 16.5s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 78, 'time_epoch': 14.26127, 'eta': 982.07807, 'eta_hours': 0.2728, 'loss': 0.21442878, 'lr': 0.00010286, 'params': 13807345, 'time_iter': 0.13982, 'mae': 0.21443, 'r2': 0.95436, 'spearmanr': 0.97767, 'mse': 0.09377, 'rmse': 0.30622}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 78, 'time_epoch': 1.31065, 'loss': 0.58538834, 'lr': 0, 'params': 13807345, 'time_iter': 0.04519, 'mae': 0.58539, 'r2': 0.66723, 'spearmanr': 0.82835, 'mse': 0.67417, 'rmse': 0.82108}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 78, 'time_epoch': 0.6834, 'loss': 0.54845508, 'lr': 0, 'params': 13807345, 'time_iter': 0.04556, 'mae': 0.54846, 'r2': 0.70503, 'spearmanr': 0.84479, 'mse': 0.58356, 'rmse': 0.76391}\n",
            "> Epoch 78: took 16.3s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 79, 'time_epoch': 13.75578, 'eta': 968.17922, 'eta_hours': 0.26894, 'loss': 0.20899789, 'lr': 0.00010143, 'params': 13807345, 'time_iter': 0.13486, 'mae': 0.209, 'r2': 0.95527, 'spearmanr': 0.9778, 'mse': 0.0919, 'rmse': 0.30315}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 79, 'time_epoch': 1.95042, 'loss': 0.58769402, 'lr': 0, 'params': 13807345, 'time_iter': 0.06726, 'mae': 0.58769, 'r2': 0.65923, 'spearmanr': 0.82254, 'mse': 0.69037, 'rmse': 0.83089}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 79, 'time_epoch': 0.68865, 'loss': 0.53699023, 'lr': 0, 'params': 13807345, 'time_iter': 0.04591, 'mae': 0.53699, 'r2': 0.70307, 'spearmanr': 0.84054, 'mse': 0.58743, 'rmse': 0.76644}\n",
            "> Epoch 79: took 16.4s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 80, 'time_epoch': 12.67667, 'eta': 953.36465, 'eta_hours': 0.26482, 'loss': 0.21117663, 'lr': 0.0001, 'params': 13807345, 'time_iter': 0.12428, 'mae': 0.21118, 'r2': 0.95733, 'spearmanr': 0.97833, 'mse': 0.08766, 'rmse': 0.29607}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 80, 'time_epoch': 1.96724, 'loss': 0.59155247, 'lr': 0, 'params': 13807345, 'time_iter': 0.06784, 'mae': 0.59155, 'r2': 0.65851, 'spearmanr': 0.82485, 'mse': 0.69183, 'rmse': 0.83176}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 80, 'time_epoch': 1.06527, 'loss': 0.54437848, 'lr': 0, 'params': 13807345, 'time_iter': 0.07102, 'mae': 0.54438, 'r2': 0.70518, 'spearmanr': 0.84446, 'mse': 0.58325, 'rmse': 0.76371}\n",
            "> Epoch 80: took 15.8s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 81, 'time_epoch': 12.91056, 'eta': 938.79619, 'eta_hours': 0.26078, 'loss': 0.21649118, 'lr': 9.857e-05, 'params': 13807345, 'time_iter': 0.12657, 'mae': 0.21649, 'r2': 0.95244, 'spearmanr': 0.97659, 'mse': 0.0977, 'rmse': 0.31258}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 81, 'time_epoch': 1.71926, 'loss': 0.59170687, 'lr': 0, 'params': 13807345, 'time_iter': 0.05928, 'mae': 0.59171, 'r2': 0.6572, 'spearmanr': 0.82284, 'mse': 0.69448, 'rmse': 0.83336}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 81, 'time_epoch': 0.96618, 'loss': 0.53543682, 'lr': 0, 'params': 13807345, 'time_iter': 0.06441, 'mae': 0.53544, 'r2': 0.70931, 'spearmanr': 0.84555, 'mse': 0.57509, 'rmse': 0.75835}\n",
            "> Epoch 81: took 15.6s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 82, 'time_epoch': 13.34717, 'eta': 924.62013, 'eta_hours': 0.25684, 'loss': 0.20520151, 'lr': 9.714e-05, 'params': 13807345, 'time_iter': 0.13085, 'mae': 0.2052, 'r2': 0.95899, 'spearmanr': 0.97985, 'mse': 0.08425, 'rmse': 0.29027}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 82, 'time_epoch': 1.31352, 'loss': 0.59394756, 'lr': 0, 'params': 13807345, 'time_iter': 0.04529, 'mae': 0.59395, 'r2': 0.65155, 'spearmanr': 0.82038, 'mse': 0.70594, 'rmse': 0.8402}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 82, 'time_epoch': 0.76328, 'loss': 0.55319047, 'lr': 0, 'params': 13807345, 'time_iter': 0.05089, 'mae': 0.55319, 'r2': 0.69057, 'spearmanr': 0.83631, 'mse': 0.61217, 'rmse': 0.78241}\n",
            "> Epoch 82: took 15.5s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 83, 'time_epoch': 14.30187, 'eta': 911.21392, 'eta_hours': 0.25311, 'loss': 0.20397493, 'lr': 9.571e-05, 'params': 13807345, 'time_iter': 0.14021, 'mae': 0.20397, 'r2': 0.95875, 'spearmanr': 0.97901, 'mse': 0.08474, 'rmse': 0.29111}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 83, 'time_epoch': 1.34798, 'loss': 0.58895911, 'lr': 0, 'params': 13807345, 'time_iter': 0.04648, 'mae': 0.58896, 'r2': 0.66339, 'spearmanr': 0.82767, 'mse': 0.68195, 'rmse': 0.8258}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 83, 'time_epoch': 0.69329, 'loss': 0.53846625, 'lr': 0, 'params': 13807345, 'time_iter': 0.04622, 'mae': 0.53847, 'r2': 0.70593, 'spearmanr': 0.8421, 'mse': 0.58177, 'rmse': 0.76274}\n",
            "> Epoch 83: took 16.4s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 84, 'time_epoch': 14.25551, 'eta': 897.75118, 'eta_hours': 0.24938, 'loss': 0.20629861, 'lr': 9.429e-05, 'params': 13807345, 'time_iter': 0.13976, 'mae': 0.2063, 'r2': 0.95869, 'spearmanr': 0.97966, 'mse': 0.08486, 'rmse': 0.29131}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 84, 'time_epoch': 1.33359, 'loss': 0.58477487, 'lr': 0, 'params': 13807345, 'time_iter': 0.04599, 'mae': 0.58477, 'r2': 0.67202, 'spearmanr': 0.82892, 'mse': 0.66447, 'rmse': 0.81515}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 84, 'time_epoch': 0.68812, 'loss': 0.5335326, 'lr': 0, 'params': 13807345, 'time_iter': 0.04587, 'mae': 0.53353, 'r2': 0.71556, 'spearmanr': 0.84489, 'mse': 0.56273, 'rmse': 0.75015}\n",
            "> Epoch 84: took 16.3s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 85, 'time_epoch': 14.60919, 'eta': 884.53321, 'eta_hours': 0.2457, 'loss': 0.20550337, 'lr': 9.286e-05, 'params': 13807345, 'time_iter': 0.14323, 'mae': 0.2055, 'r2': 0.95932, 'spearmanr': 0.98023, 'mse': 0.08357, 'rmse': 0.28908}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 85, 'time_epoch': 1.30383, 'loss': 0.58606777, 'lr': 0, 'params': 13807345, 'time_iter': 0.04496, 'mae': 0.58607, 'r2': 0.66544, 'spearmanr': 0.82924, 'mse': 0.6778, 'rmse': 0.82329}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 85, 'time_epoch': 0.68528, 'loss': 0.54692983, 'lr': 0, 'params': 13807345, 'time_iter': 0.04569, 'mae': 0.54693, 'r2': 0.69781, 'spearmanr': 0.84112, 'mse': 0.59783, 'rmse': 0.7732}\n",
            "> Epoch 85: took 16.6s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 86, 'time_epoch': 14.29709, 'eta': 871.05726, 'eta_hours': 0.24196, 'loss': 0.19959696, 'lr': 9.143e-05, 'params': 13807345, 'time_iter': 0.14017, 'mae': 0.1996, 'r2': 0.96087, 'spearmanr': 0.98088, 'mse': 0.08039, 'rmse': 0.28353}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 86, 'time_epoch': 1.41133, 'loss': 0.5863359, 'lr': 0, 'params': 13807345, 'time_iter': 0.04867, 'mae': 0.58634, 'r2': 0.66751, 'spearmanr': 0.82827, 'mse': 0.67359, 'rmse': 0.82073}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 86, 'time_epoch': 0.69206, 'loss': 0.53826095, 'lr': 0, 'params': 13807345, 'time_iter': 0.04614, 'mae': 0.53826, 'r2': 0.70447, 'spearmanr': 0.84229, 'mse': 0.58466, 'rmse': 0.76463}\n",
            "> Epoch 86: took 16.4s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 87, 'time_epoch': 13.13111, 'eta': 856.74115, 'eta_hours': 0.23798, 'loss': 0.19973622, 'lr': 9e-05, 'params': 13807345, 'time_iter': 0.12874, 'mae': 0.19974, 'r2': 0.96056, 'spearmanr': 0.9804, 'mse': 0.08103, 'rmse': 0.28465}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 87, 'time_epoch': 1.95335, 'loss': 0.59654156, 'lr': 0, 'params': 13807345, 'time_iter': 0.06736, 'mae': 0.59654, 'r2': 0.65861, 'spearmanr': 0.82387, 'mse': 0.69163, 'rmse': 0.83164}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 87, 'time_epoch': 1.03867, 'loss': 0.53216126, 'lr': 0, 'params': 13807345, 'time_iter': 0.06924, 'mae': 0.53216, 'r2': 0.7154, 'spearmanr': 0.84729, 'mse': 0.56304, 'rmse': 0.75036}\n",
            "> Epoch 87: took 16.2s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 88, 'time_epoch': 12.32426, 'eta': 841.89867, 'eta_hours': 0.23386, 'loss': 0.20034343, 'lr': 8.857e-05, 'params': 13807345, 'time_iter': 0.12083, 'mae': 0.20034, 'r2': 0.95921, 'spearmanr': 0.98028, 'mse': 0.08381, 'rmse': 0.2895}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 88, 'time_epoch': 1.7633, 'loss': 0.58555749, 'lr': 0, 'params': 13807345, 'time_iter': 0.0608, 'mae': 0.58556, 'r2': 0.66918, 'spearmanr': 0.82396, 'mse': 0.67022, 'rmse': 0.81867}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 88, 'time_epoch': 1.03637, 'loss': 0.52425509, 'lr': 0, 'params': 13807345, 'time_iter': 0.06909, 'mae': 0.52426, 'r2': 0.71646, 'spearmanr': 0.84458, 'mse': 0.56095, 'rmse': 0.74897}\n",
            "> Epoch 88: took 15.2s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 89, 'time_epoch': 13.01029, 'eta': 827.5695, 'eta_hours': 0.22988, 'loss': 0.19622861, 'lr': 8.714e-05, 'params': 13807345, 'time_iter': 0.12755, 'mae': 0.19623, 'r2': 0.96255, 'spearmanr': 0.98139, 'mse': 0.07695, 'rmse': 0.27739}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 89, 'time_epoch': 1.32321, 'loss': 0.58687859, 'lr': 0, 'params': 13807345, 'time_iter': 0.04563, 'mae': 0.58688, 'r2': 0.66455, 'spearmanr': 0.82386, 'mse': 0.6796, 'rmse': 0.82438}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 89, 'time_epoch': 0.93996, 'loss': 0.53110054, 'lr': 0, 'params': 13807345, 'time_iter': 0.06266, 'mae': 0.5311, 'r2': 0.71409, 'spearmanr': 0.84508, 'mse': 0.56563, 'rmse': 0.75208}\n",
            "> Epoch 89: took 15.3s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 90, 'time_epoch': 14.35553, 'eta': 814.1415, 'eta_hours': 0.22615, 'loss': 0.1952347, 'lr': 8.571e-05, 'params': 13807345, 'time_iter': 0.14074, 'mae': 0.19523, 'r2': 0.96216, 'spearmanr': 0.98143, 'mse': 0.07774, 'rmse': 0.27882}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 90, 'time_epoch': 1.34054, 'loss': 0.58650985, 'lr': 0, 'params': 13807345, 'time_iter': 0.04623, 'mae': 0.58651, 'r2': 0.66884, 'spearmanr': 0.82661, 'mse': 0.67091, 'rmse': 0.81909}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 90, 'time_epoch': 0.69305, 'loss': 0.53659436, 'lr': 0, 'params': 13807345, 'time_iter': 0.0462, 'mae': 0.53659, 'r2': 0.70298, 'spearmanr': 0.83864, 'mse': 0.5876, 'rmse': 0.76655}\n",
            "> Epoch 90: took 16.4s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 91, 'time_epoch': 14.3654, 'eta': 800.69956, 'eta_hours': 0.22242, 'loss': 0.19575303, 'lr': 8.429e-05, 'params': 13807345, 'time_iter': 0.14084, 'mae': 0.19575, 'r2': 0.96166, 'spearmanr': 0.98149, 'mse': 0.07877, 'rmse': 0.28066}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 91, 'time_epoch': 1.33133, 'loss': 0.58994184, 'lr': 0, 'params': 13807345, 'time_iter': 0.04591, 'mae': 0.58994, 'r2': 0.66266, 'spearmanr': 0.82251, 'mse': 0.68342, 'rmse': 0.82669}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 91, 'time_epoch': 0.68608, 'loss': 0.53973158, 'lr': 0, 'params': 13807345, 'time_iter': 0.04574, 'mae': 0.53973, 'r2': 0.70806, 'spearmanr': 0.8434, 'mse': 0.57755, 'rmse': 0.75997}\n",
            "> Epoch 91: took 16.4s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 92, 'time_epoch': 14.34468, 'eta': 787.22507, 'eta_hours': 0.21867, 'loss': 0.19145908, 'lr': 8.286e-05, 'params': 13807345, 'time_iter': 0.14063, 'mae': 0.19146, 'r2': 0.96348, 'spearmanr': 0.98204, 'mse': 0.07503, 'rmse': 0.27392}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 92, 'time_epoch': 1.30932, 'loss': 0.57931551, 'lr': 0, 'params': 13807345, 'time_iter': 0.04515, 'mae': 0.57932, 'r2': 0.67849, 'spearmanr': 0.82901, 'mse': 0.65135, 'rmse': 0.80706}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 92, 'time_epoch': 0.69181, 'loss': 0.54142884, 'lr': 0, 'params': 13807345, 'time_iter': 0.04612, 'mae': 0.54143, 'r2': 0.70976, 'spearmanr': 0.84079, 'mse': 0.57419, 'rmse': 0.75775}\n",
            "> Epoch 92: took 16.4s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 93, 'time_epoch': 14.43073, 'eta': 773.78332, 'eta_hours': 0.21494, 'loss': 0.18756248, 'lr': 8.143e-05, 'params': 13807345, 'time_iter': 0.14148, 'mae': 0.18756, 'r2': 0.96509, 'spearmanr': 0.9827, 'mse': 0.07172, 'rmse': 0.26781}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 93, 'time_epoch': 1.33401, 'loss': 0.5895471, 'lr': 0, 'params': 13807345, 'time_iter': 0.046, 'mae': 0.58955, 'r2': 0.66463, 'spearmanr': 0.82811, 'mse': 0.67944, 'rmse': 0.82428}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 93, 'time_epoch': 0.70945, 'loss': 0.54067428, 'lr': 0, 'params': 13807345, 'time_iter': 0.0473, 'mae': 0.54067, 'r2': 0.71288, 'spearmanr': 0.84551, 'mse': 0.56802, 'rmse': 0.75367}\n",
            "> Epoch 93: took 16.5s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 94, 'time_epoch': 13.85438, 'eta': 759.98708, 'eta_hours': 0.21111, 'loss': 0.19027569, 'lr': 8e-05, 'params': 13807345, 'time_iter': 0.13583, 'mae': 0.19028, 'r2': 0.96407, 'spearmanr': 0.9823, 'mse': 0.07383, 'rmse': 0.27171}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 94, 'time_epoch': 1.96747, 'loss': 0.58637401, 'lr': 0, 'params': 13807345, 'time_iter': 0.06784, 'mae': 0.58637, 'r2': 0.67088, 'spearmanr': 0.82713, 'mse': 0.66678, 'rmse': 0.81656}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 94, 'time_epoch': 0.74802, 'loss': 0.54230643, 'lr': 0, 'params': 13807345, 'time_iter': 0.04987, 'mae': 0.54231, 'r2': 0.7098, 'spearmanr': 0.8432, 'mse': 0.57411, 'rmse': 0.7577}\n",
            "> Epoch 94: took 16.6s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 95, 'time_epoch': 12.50674, 'eta': 745.43158, 'eta_hours': 0.20706, 'loss': 0.18878493, 'lr': 7.857e-05, 'params': 13807345, 'time_iter': 0.12262, 'mae': 0.18878, 'r2': 0.96454, 'spearmanr': 0.98272, 'mse': 0.07284, 'rmse': 0.26989}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 95, 'time_epoch': 1.92355, 'loss': 0.58605562, 'lr': 0, 'params': 13807345, 'time_iter': 0.06633, 'mae': 0.58606, 'r2': 0.66913, 'spearmanr': 0.82783, 'mse': 0.67032, 'rmse': 0.81873}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 95, 'time_epoch': 1.05061, 'loss': 0.54138339, 'lr': 0, 'params': 13807345, 'time_iter': 0.07004, 'mae': 0.54138, 'r2': 0.71154, 'spearmanr': 0.84595, 'mse': 0.57068, 'rmse': 0.75543}\n",
            "> Epoch 95: took 15.5s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 96, 'time_epoch': 12.57981, 'eta': 730.95824, 'eta_hours': 0.20304, 'loss': 0.18778273, 'lr': 7.714e-05, 'params': 13807345, 'time_iter': 0.12333, 'mae': 0.18778, 'r2': 0.96371, 'spearmanr': 0.98149, 'mse': 0.07455, 'rmse': 0.27304}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 96, 'time_epoch': 1.74772, 'loss': 0.5910121, 'lr': 0, 'params': 13807345, 'time_iter': 0.06027, 'mae': 0.59101, 'r2': 0.66527, 'spearmanr': 0.82369, 'mse': 0.67814, 'rmse': 0.8235}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 96, 'time_epoch': 0.9086, 'loss': 0.54115744, 'lr': 0, 'params': 13807345, 'time_iter': 0.06057, 'mae': 0.54116, 'r2': 0.71002, 'spearmanr': 0.84225, 'mse': 0.57369, 'rmse': 0.75742}\n",
            "> Epoch 96: took 15.3s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 97, 'time_epoch': 13.63592, 'eta': 717.08394, 'eta_hours': 0.19919, 'loss': 0.18656561, 'lr': 7.571e-05, 'params': 13807345, 'time_iter': 0.13369, 'mae': 0.18657, 'r2': 0.96614, 'spearmanr': 0.98334, 'mse': 0.06957, 'rmse': 0.26375}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 97, 'time_epoch': 1.33663, 'loss': 0.58627999, 'lr': 0, 'params': 13807345, 'time_iter': 0.04609, 'mae': 0.58628, 'r2': 0.66803, 'spearmanr': 0.82523, 'mse': 0.67255, 'rmse': 0.82009}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 97, 'time_epoch': 0.68869, 'loss': 0.53895699, 'lr': 0, 'params': 13807345, 'time_iter': 0.04591, 'mae': 0.53896, 'r2': 0.71026, 'spearmanr': 0.84081, 'mse': 0.57321, 'rmse': 0.75711}\n",
            "> Epoch 97: took 15.7s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 98, 'time_epoch': 14.44353, 'eta': 703.63049, 'eta_hours': 0.19545, 'loss': 0.18936165, 'lr': 7.429e-05, 'params': 13807345, 'time_iter': 0.1416, 'mae': 0.18936, 'r2': 0.96455, 'spearmanr': 0.9828, 'mse': 0.07283, 'rmse': 0.26987}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 98, 'time_epoch': 1.35444, 'loss': 0.58575897, 'lr': 0, 'params': 13807345, 'time_iter': 0.0467, 'mae': 0.58576, 'r2': 0.66769, 'spearmanr': 0.82381, 'mse': 0.67323, 'rmse': 0.8205}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 98, 'time_epoch': 0.68201, 'loss': 0.53477735, 'lr': 0, 'params': 13807345, 'time_iter': 0.04547, 'mae': 0.53478, 'r2': 0.71435, 'spearmanr': 0.84512, 'mse': 0.56511, 'rmse': 0.75174}\n",
            "> Epoch 98: took 16.5s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 99, 'time_epoch': 14.40296, 'eta': 690.13696, 'eta_hours': 0.1917, 'loss': 0.18768641, 'lr': 7.286e-05, 'params': 13807345, 'time_iter': 0.14121, 'mae': 0.18769, 'r2': 0.96557, 'spearmanr': 0.98319, 'mse': 0.07074, 'rmse': 0.26596}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 99, 'time_epoch': 1.30592, 'loss': 0.59097366, 'lr': 0, 'params': 13807345, 'time_iter': 0.04503, 'mae': 0.59097, 'r2': 0.66748, 'spearmanr': 0.8301, 'mse': 0.67367, 'rmse': 0.82077}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 99, 'time_epoch': 0.68698, 'loss': 0.54431595, 'lr': 0, 'params': 13807345, 'time_iter': 0.0458, 'mae': 0.54432, 'r2': 0.70718, 'spearmanr': 0.84212, 'mse': 0.57929, 'rmse': 0.76111}\n",
            "> Epoch 99: took 16.4s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 100, 'time_epoch': 15.16147, 'eta': 676.9934, 'eta_hours': 0.18805, 'loss': 0.18448134, 'lr': 7.143e-05, 'params': 13807345, 'time_iter': 0.14864, 'mae': 0.18448, 'r2': 0.96578, 'spearmanr': 0.98294, 'mse': 0.07029, 'rmse': 0.26513}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 100, 'time_epoch': 1.30688, 'loss': 0.59427205, 'lr': 0, 'params': 13807345, 'time_iter': 0.04506, 'mae': 0.59427, 'r2': 0.6638, 'spearmanr': 0.82823, 'mse': 0.68111, 'rmse': 0.82529}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 100, 'time_epoch': 0.69577, 'loss': 0.54421744, 'lr': 0, 'params': 13807345, 'time_iter': 0.04638, 'mae': 0.54422, 'r2': 0.70645, 'spearmanr': 0.84179, 'mse': 0.58075, 'rmse': 0.76207}\n",
            "> Epoch 100: took 17.2s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 101, 'time_epoch': 14.32657, 'eta': 663.41739, 'eta_hours': 0.18428, 'loss': 0.18668921, 'lr': 7e-05, 'params': 13807345, 'time_iter': 0.14046, 'mae': 0.18669, 'r2': 0.96673, 'spearmanr': 0.98371, 'mse': 0.06835, 'rmse': 0.26144}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 101, 'time_epoch': 1.32977, 'loss': 0.58102847, 'lr': 0, 'params': 13807345, 'time_iter': 0.04585, 'mae': 0.58103, 'r2': 0.67482, 'spearmanr': 0.83018, 'mse': 0.65878, 'rmse': 0.81165}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 101, 'time_epoch': 0.67361, 'loss': 0.53830367, 'lr': 0, 'params': 13807345, 'time_iter': 0.04491, 'mae': 0.5383, 'r2': 0.70915, 'spearmanr': 0.84195, 'mse': 0.57539, 'rmse': 0.75855}\n",
            "> Epoch 101: took 16.4s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 102, 'time_epoch': 14.15352, 'eta': 649.74784, 'eta_hours': 0.18049, 'loss': 0.18090987, 'lr': 6.857e-05, 'params': 13807345, 'time_iter': 0.13876, 'mae': 0.18091, 'r2': 0.96869, 'spearmanr': 0.98462, 'mse': 0.06433, 'rmse': 0.25363}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 102, 'time_epoch': 1.69268, 'loss': 0.58023582, 'lr': 0, 'params': 13807345, 'time_iter': 0.05837, 'mae': 0.58024, 'r2': 0.67496, 'spearmanr': 0.83248, 'mse': 0.6585, 'rmse': 0.81148}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 102, 'time_epoch': 0.6759, 'loss': 0.53872921, 'lr': 0, 'params': 13807345, 'time_iter': 0.04506, 'mae': 0.53873, 'r2': 0.71183, 'spearmanr': 0.84453, 'mse': 0.5701, 'rmse': 0.75505}\n",
            "> Epoch 102: took 16.6s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 103, 'time_epoch': 12.91015, 'eta': 635.51902, 'eta_hours': 0.17653, 'loss': 0.17927057, 'lr': 6.714e-05, 'params': 13807345, 'time_iter': 0.12657, 'mae': 0.17927, 'r2': 0.96834, 'spearmanr': 0.98448, 'mse': 0.06505, 'rmse': 0.25505}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 103, 'time_epoch': 1.97811, 'loss': 0.58383485, 'lr': 0, 'params': 13807345, 'time_iter': 0.06821, 'mae': 0.58383, 'r2': 0.66997, 'spearmanr': 0.82999, 'mse': 0.66862, 'rmse': 0.81769}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 103, 'time_epoch': 1.0395, 'loss': 0.54235212, 'lr': 0, 'params': 13807345, 'time_iter': 0.0693, 'mae': 0.54235, 'r2': 0.70298, 'spearmanr': 0.84307, 'mse': 0.5876, 'rmse': 0.76655}\n",
            "> Epoch 103: took 16.0s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 104, 'time_epoch': 12.38991, 'eta': 621.09237, 'eta_hours': 0.17253, 'loss': 0.17788107, 'lr': 6.571e-05, 'params': 13807345, 'time_iter': 0.12147, 'mae': 0.17788, 'r2': 0.96935, 'spearmanr': 0.98504, 'mse': 0.06296, 'rmse': 0.25092}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 104, 'time_epoch': 1.78412, 'loss': 0.57795362, 'lr': 0, 'params': 13807345, 'time_iter': 0.06152, 'mae': 0.57795, 'r2': 0.67739, 'spearmanr': 0.83213, 'mse': 0.65358, 'rmse': 0.80845}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 104, 'time_epoch': 0.96676, 'loss': 0.53682381, 'lr': 0, 'params': 13807345, 'time_iter': 0.06445, 'mae': 0.53682, 'r2': 0.70833, 'spearmanr': 0.84159, 'mse': 0.57703, 'rmse': 0.75963}\n",
            "> Epoch 104: took 15.2s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 105, 'time_epoch': 13.57657, 'eta': 607.19672, 'eta_hours': 0.16867, 'loss': 0.17214698, 'lr': 6.429e-05, 'params': 13807345, 'time_iter': 0.1331, 'mae': 0.17215, 'r2': 0.97025, 'spearmanr': 0.98525, 'mse': 0.06113, 'rmse': 0.24724}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 105, 'time_epoch': 1.31588, 'loss': 0.58865392, 'lr': 0, 'params': 13807345, 'time_iter': 0.04538, 'mae': 0.58865, 'r2': 0.66461, 'spearmanr': 0.82962, 'mse': 0.67948, 'rmse': 0.8243}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 105, 'time_epoch': 0.8619, 'loss': 0.53853815, 'lr': 0, 'params': 13807345, 'time_iter': 0.05746, 'mae': 0.53854, 'r2': 0.70713, 'spearmanr': 0.846, 'mse': 0.5794, 'rmse': 0.76119}\n",
            "> Epoch 105: took 15.8s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 106, 'time_epoch': 14.29833, 'eta': 593.59709, 'eta_hours': 0.16489, 'loss': 0.17611823, 'lr': 6.286e-05, 'params': 13807345, 'time_iter': 0.14018, 'mae': 0.17612, 'r2': 0.9698, 'spearmanr': 0.98511, 'mse': 0.06205, 'rmse': 0.24909}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 106, 'time_epoch': 1.30326, 'loss': 0.58540585, 'lr': 0, 'params': 13807345, 'time_iter': 0.04494, 'mae': 0.58541, 'r2': 0.66949, 'spearmanr': 0.8278, 'mse': 0.66959, 'rmse': 0.81828}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 106, 'time_epoch': 0.67217, 'loss': 0.54223, 'lr': 0, 'params': 13807345, 'time_iter': 0.04481, 'mae': 0.54223, 'r2': 0.70628, 'spearmanr': 0.84163, 'mse': 0.58109, 'rmse': 0.76229}\n",
            "> Epoch 106: took 16.3s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 107, 'time_epoch': 14.54078, 'eta': 580.0788, 'eta_hours': 0.16113, 'loss': 0.17868508, 'lr': 6.143e-05, 'params': 13807345, 'time_iter': 0.14256, 'mae': 0.17869, 'r2': 0.96932, 'spearmanr': 0.98454, 'mse': 0.06303, 'rmse': 0.25106}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 107, 'time_epoch': 1.31281, 'loss': 0.57977804, 'lr': 0, 'params': 13807345, 'time_iter': 0.04527, 'mae': 0.57978, 'r2': 0.67249, 'spearmanr': 0.83048, 'mse': 0.66351, 'rmse': 0.81456}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 107, 'time_epoch': 0.68245, 'loss': 0.53376179, 'lr': 0, 'params': 13807345, 'time_iter': 0.0455, 'mae': 0.53376, 'r2': 0.71684, 'spearmanr': 0.84808, 'mse': 0.56018, 'rmse': 0.74845}\n",
            "> Epoch 107: took 16.6s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 108, 'time_epoch': 14.31238, 'eta': 566.45584, 'eta_hours': 0.15735, 'loss': 0.17231828, 'lr': 6e-05, 'params': 13807345, 'time_iter': 0.14032, 'mae': 0.17232, 'r2': 0.97083, 'spearmanr': 0.98527, 'mse': 0.05992, 'rmse': 0.24478}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 108, 'time_epoch': 1.29921, 'loss': 0.58428597, 'lr': 0, 'params': 13807345, 'time_iter': 0.0448, 'mae': 0.58429, 'r2': 0.66697, 'spearmanr': 0.82803, 'mse': 0.67469, 'rmse': 0.82139}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 108, 'time_epoch': 0.69534, 'loss': 0.54330195, 'lr': 0, 'params': 13807345, 'time_iter': 0.04636, 'mae': 0.5433, 'r2': 0.70728, 'spearmanr': 0.84375, 'mse': 0.5791, 'rmse': 0.76099}\n",
            "> Epoch 108: took 16.3s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 109, 'time_epoch': 14.36625, 'eta': 552.83994, 'eta_hours': 0.15357, 'loss': 0.17536676, 'lr': 5.857e-05, 'params': 13807345, 'time_iter': 0.14085, 'mae': 0.17537, 'r2': 0.96854, 'spearmanr': 0.98437, 'mse': 0.06464, 'rmse': 0.25424}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 109, 'time_epoch': 1.34547, 'loss': 0.58408478, 'lr': 0, 'params': 13807345, 'time_iter': 0.0464, 'mae': 0.58408, 'r2': 0.66722, 'spearmanr': 0.82829, 'mse': 0.67419, 'rmse': 0.82109}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 109, 'time_epoch': 0.67073, 'loss': 0.54481405, 'lr': 0, 'params': 13807345, 'time_iter': 0.04472, 'mae': 0.54481, 'r2': 0.70477, 'spearmanr': 0.84166, 'mse': 0.58406, 'rmse': 0.76424}\n",
            "> Epoch 109: took 16.4s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 110, 'time_epoch': 13.35778, 'eta': 538.85619, 'eta_hours': 0.14968, 'loss': 0.17400001, 'lr': 5.714e-05, 'params': 13807345, 'time_iter': 0.13096, 'mae': 0.174, 'r2': 0.96957, 'spearmanr': 0.98474, 'mse': 0.06251, 'rmse': 0.25002}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 110, 'time_epoch': 1.98227, 'loss': 0.57885119, 'lr': 0, 'params': 13807345, 'time_iter': 0.06835, 'mae': 0.57885, 'r2': 0.6708, 'spearmanr': 0.82657, 'mse': 0.66693, 'rmse': 0.81666}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 110, 'time_epoch': 1.07588, 'loss': 0.54161697, 'lr': 0, 'params': 13807345, 'time_iter': 0.07173, 'mae': 0.54162, 'r2': 0.71017, 'spearmanr': 0.84191, 'mse': 0.57338, 'rmse': 0.75722}\n",
            "> Epoch 110: took 16.5s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 111, 'time_epoch': 12.69782, 'eta': 524.6597, 'eta_hours': 0.14574, 'loss': 0.16963828, 'lr': 5.571e-05, 'params': 13807345, 'time_iter': 0.12449, 'mae': 0.16964, 'r2': 0.97198, 'spearmanr': 0.9861, 'mse': 0.05757, 'rmse': 0.23993}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 111, 'time_epoch': 1.877, 'loss': 0.58286555, 'lr': 0, 'params': 13807345, 'time_iter': 0.06472, 'mae': 0.58287, 'r2': 0.67212, 'spearmanr': 0.83204, 'mse': 0.66426, 'rmse': 0.81502}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 111, 'time_epoch': 1.02913, 'loss': 0.54431198, 'lr': 0, 'params': 13807345, 'time_iter': 0.06861, 'mae': 0.54431, 'r2': 0.70379, 'spearmanr': 0.84068, 'mse': 0.58601, 'rmse': 0.76551}\n",
            "> Epoch 111: took 15.6s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 112, 'time_epoch': 12.75361, 'eta': 510.50801, 'eta_hours': 0.14181, 'loss': 0.17374331, 'lr': 5.429e-05, 'params': 13807345, 'time_iter': 0.12504, 'mae': 0.17374, 'r2': 0.97081, 'spearmanr': 0.98561, 'mse': 0.05998, 'rmse': 0.2449}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 112, 'time_epoch': 1.62123, 'loss': 0.58933165, 'lr': 0, 'params': 13807345, 'time_iter': 0.0559, 'mae': 0.58933, 'r2': 0.6636, 'spearmanr': 0.82911, 'mse': 0.68152, 'rmse': 0.82554}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 112, 'time_epoch': 0.93367, 'loss': 0.5484714, 'lr': 0, 'params': 13807345, 'time_iter': 0.06224, 'mae': 0.54847, 'r2': 0.70038, 'spearmanr': 0.8417, 'mse': 0.59275, 'rmse': 0.76991}\n",
            "> Epoch 112: took 15.4s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 113, 'time_epoch': 13.85409, 'eta': 496.72836, 'eta_hours': 0.13798, 'loss': 0.17136819, 'lr': 5.286e-05, 'params': 13807345, 'time_iter': 0.13582, 'mae': 0.17137, 'r2': 0.97163, 'spearmanr': 0.98627, 'mse': 0.05828, 'rmse': 0.24142}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 113, 'time_epoch': 1.32273, 'loss': 0.57954771, 'lr': 0, 'params': 13807345, 'time_iter': 0.04561, 'mae': 0.57955, 'r2': 0.67387, 'spearmanr': 0.83028, 'mse': 0.66071, 'rmse': 0.81284}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 113, 'time_epoch': 0.67794, 'loss': 0.5375733, 'lr': 0, 'params': 13807345, 'time_iter': 0.0452, 'mae': 0.53757, 'r2': 0.71101, 'spearmanr': 0.84163, 'mse': 0.57173, 'rmse': 0.75613}\n",
            "> Epoch 113: took 15.9s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 114, 'time_epoch': 14.39908, 'eta': 483.11328, 'eta_hours': 0.1342, 'loss': 0.16457752, 'lr': 5.143e-05, 'params': 13807345, 'time_iter': 0.14117, 'mae': 0.16458, 'r2': 0.97425, 'spearmanr': 0.98717, 'mse': 0.0529, 'rmse': 0.23001}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 114, 'time_epoch': 1.32215, 'loss': 0.58318007, 'lr': 0, 'params': 13807345, 'time_iter': 0.04559, 'mae': 0.58318, 'r2': 0.66943, 'spearmanr': 0.82885, 'mse': 0.66971, 'rmse': 0.81836}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 114, 'time_epoch': 0.67199, 'loss': 0.53772932, 'lr': 0, 'params': 13807345, 'time_iter': 0.0448, 'mae': 0.53773, 'r2': 0.71148, 'spearmanr': 0.84462, 'mse': 0.57079, 'rmse': 0.75551}\n",
            "> Epoch 114: took 16.4s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 115, 'time_epoch': 14.54496, 'eta': 469.52745, 'eta_hours': 0.13042, 'loss': 0.16656965, 'lr': 5e-05, 'params': 13807345, 'time_iter': 0.1426, 'mae': 0.16657, 'r2': 0.97335, 'spearmanr': 0.98693, 'mse': 0.05475, 'rmse': 0.23399}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 115, 'time_epoch': 1.34658, 'loss': 0.58155155, 'lr': 0, 'params': 13807345, 'time_iter': 0.04643, 'mae': 0.58155, 'r2': 0.67139, 'spearmanr': 0.82851, 'mse': 0.66574, 'rmse': 0.81593}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 115, 'time_epoch': 0.69379, 'loss': 0.53806553, 'lr': 0, 'params': 13807345, 'time_iter': 0.04625, 'mae': 0.53807, 'r2': 0.70888, 'spearmanr': 0.84254, 'mse': 0.57594, 'rmse': 0.75891}\n",
            "> Epoch 115: took 16.6s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 116, 'time_epoch': 14.44433, 'eta': 455.89683, 'eta_hours': 0.12664, 'loss': 0.16405649, 'lr': 4.857e-05, 'params': 13807345, 'time_iter': 0.14161, 'mae': 0.16406, 'r2': 0.97372, 'spearmanr': 0.98709, 'mse': 0.05399, 'rmse': 0.23237}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 116, 'time_epoch': 1.35149, 'loss': 0.59224594, 'lr': 0, 'params': 13807345, 'time_iter': 0.0466, 'mae': 0.59225, 'r2': 0.651, 'spearmanr': 0.82137, 'mse': 0.70705, 'rmse': 0.84086}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 116, 'time_epoch': 0.67987, 'loss': 0.5542787, 'lr': 0, 'params': 13807345, 'time_iter': 0.04532, 'mae': 0.55428, 'r2': 0.68974, 'spearmanr': 0.83204, 'mse': 0.6138, 'rmse': 0.78345}\n",
            "> Epoch 116: took 16.5s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 117, 'time_epoch': 14.08339, 'eta': 442.15455, 'eta_hours': 0.12282, 'loss': 0.17002164, 'lr': 4.714e-05, 'params': 13807345, 'time_iter': 0.13807, 'mae': 0.17002, 'r2': 0.9725, 'spearmanr': 0.98656, 'mse': 0.05649, 'rmse': 0.23768}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 117, 'time_epoch': 1.69952, 'loss': 0.57842981, 'lr': 0, 'params': 13807345, 'time_iter': 0.0586, 'mae': 0.57843, 'r2': 0.67305, 'spearmanr': 0.83175, 'mse': 0.66238, 'rmse': 0.81387}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 117, 'time_epoch': 0.68025, 'loss': 0.53827533, 'lr': 0, 'params': 13807345, 'time_iter': 0.04535, 'mae': 0.53828, 'r2': 0.71563, 'spearmanr': 0.84629, 'mse': 0.56259, 'rmse': 0.75006}\n",
            "> Epoch 117: took 16.5s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 118, 'time_epoch': 13.32585, 'eta': 428.20919, 'eta_hours': 0.11895, 'loss': 0.16943724, 'lr': 4.571e-05, 'params': 13807345, 'time_iter': 0.13065, 'mae': 0.16944, 'r2': 0.97178, 'spearmanr': 0.98631, 'mse': 0.05798, 'rmse': 0.24079}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 118, 'time_epoch': 1.9831, 'loss': 0.58579847, 'lr': 0, 'params': 13807345, 'time_iter': 0.06838, 'mae': 0.5858, 'r2': 0.66642, 'spearmanr': 0.83029, 'mse': 0.67581, 'rmse': 0.82207}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 118, 'time_epoch': 1.07761, 'loss': 0.53981636, 'lr': 0, 'params': 13807345, 'time_iter': 0.07184, 'mae': 0.53982, 'r2': 0.71294, 'spearmanr': 0.84663, 'mse': 0.5679, 'rmse': 0.75359}\n",
            "> Epoch 118: took 16.4s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 119, 'time_epoch': 12.86391, 'eta': 414.15866, 'eta_hours': 0.11504, 'loss': 0.16318522, 'lr': 4.429e-05, 'params': 13807345, 'time_iter': 0.12612, 'mae': 0.16319, 'r2': 0.97392, 'spearmanr': 0.98703, 'mse': 0.05357, 'rmse': 0.23146}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 119, 'time_epoch': 1.76157, 'loss': 0.57818217, 'lr': 0, 'params': 13807345, 'time_iter': 0.06074, 'mae': 0.57818, 'r2': 0.6749, 'spearmanr': 0.83228, 'mse': 0.65863, 'rmse': 0.81156}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 119, 'time_epoch': 1.0325, 'loss': 0.54136888, 'lr': 0, 'params': 13807345, 'time_iter': 0.06883, 'mae': 0.54137, 'r2': 0.70987, 'spearmanr': 0.84471, 'mse': 0.57398, 'rmse': 0.75761}\n",
            "> Epoch 119: took 15.7s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 120, 'time_epoch': 13.0856, 'eta': 400.18089, 'eta_hours': 0.11116, 'loss': 0.16398155, 'lr': 4.286e-05, 'params': 13807345, 'time_iter': 0.12829, 'mae': 0.16398, 'r2': 0.97345, 'spearmanr': 0.9872, 'mse': 0.05455, 'rmse': 0.23356}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 120, 'time_epoch': 1.45206, 'loss': 0.59368323, 'lr': 0, 'params': 13807345, 'time_iter': 0.05007, 'mae': 0.59368, 'r2': 0.65148, 'spearmanr': 0.82025, 'mse': 0.70607, 'rmse': 0.84028}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 120, 'time_epoch': 0.91791, 'loss': 0.55204351, 'lr': 0, 'params': 13807345, 'time_iter': 0.06119, 'mae': 0.55204, 'r2': 0.68492, 'spearmanr': 0.8309, 'mse': 0.62334, 'rmse': 0.78952}\n",
            "> Epoch 120: took 15.5s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 121, 'time_epoch': 14.1442, 'eta': 386.4607, 'eta_hours': 0.10735, 'loss': 0.1619751, 'lr': 4.143e-05, 'params': 13807345, 'time_iter': 0.13867, 'mae': 0.16198, 'r2': 0.97406, 'spearmanr': 0.9874, 'mse': 0.0533, 'rmse': 0.23086}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 121, 'time_epoch': 1.34622, 'loss': 0.58405223, 'lr': 0, 'params': 13807345, 'time_iter': 0.04642, 'mae': 0.58405, 'r2': 0.66745, 'spearmanr': 0.82827, 'mse': 0.67373, 'rmse': 0.82081}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 121, 'time_epoch': 0.69287, 'loss': 0.53596386, 'lr': 0, 'params': 13807345, 'time_iter': 0.04619, 'mae': 0.53596, 'r2': 0.70965, 'spearmanr': 0.84417, 'mse': 0.57442, 'rmse': 0.7579}\n",
            "> Epoch 121: took 16.2s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 122, 'time_epoch': 14.45935, 'eta': 372.80279, 'eta_hours': 0.10356, 'loss': 0.1629952, 'lr': 4e-05, 'params': 13807345, 'time_iter': 0.14176, 'mae': 0.163, 'r2': 0.97344, 'spearmanr': 0.98676, 'mse': 0.05456, 'rmse': 0.23358}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 122, 'time_epoch': 1.36917, 'loss': 0.57733869, 'lr': 0, 'params': 13807345, 'time_iter': 0.04721, 'mae': 0.57734, 'r2': 0.67452, 'spearmanr': 0.83077, 'mse': 0.65939, 'rmse': 0.81203}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 122, 'time_epoch': 0.69363, 'loss': 0.53373264, 'lr': 0, 'params': 13807345, 'time_iter': 0.04624, 'mae': 0.53373, 'r2': 0.7154, 'spearmanr': 0.84419, 'mse': 0.56303, 'rmse': 0.75035}\n",
            "> Epoch 122: took 16.6s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 123, 'time_epoch': 14.50159, 'eta': 359.14081, 'eta_hours': 0.09976, 'loss': 0.15809438, 'lr': 3.857e-05, 'params': 13807345, 'time_iter': 0.14217, 'mae': 0.15809, 'r2': 0.97558, 'spearmanr': 0.98815, 'mse': 0.05017, 'rmse': 0.22399}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 123, 'time_epoch': 1.31138, 'loss': 0.58418849, 'lr': 0, 'params': 13807345, 'time_iter': 0.04522, 'mae': 0.58419, 'r2': 0.66993, 'spearmanr': 0.82966, 'mse': 0.66869, 'rmse': 0.81774}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 123, 'time_epoch': 0.68824, 'loss': 0.5373835, 'lr': 0, 'params': 13807345, 'time_iter': 0.04588, 'mae': 0.53738, 'r2': 0.70932, 'spearmanr': 0.84291, 'mse': 0.57507, 'rmse': 0.75834}\n",
            "> Epoch 123: took 16.5s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 124, 'time_epoch': 14.36116, 'eta': 345.43731, 'eta_hours': 0.09595, 'loss': 0.16491147, 'lr': 3.714e-05, 'params': 13807345, 'time_iter': 0.1408, 'mae': 0.16491, 'r2': 0.97344, 'spearmanr': 0.98689, 'mse': 0.05456, 'rmse': 0.23359}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 124, 'time_epoch': 1.32553, 'loss': 0.58429727, 'lr': 0, 'params': 13807345, 'time_iter': 0.04571, 'mae': 0.5843, 'r2': 0.66798, 'spearmanr': 0.82894, 'mse': 0.67264, 'rmse': 0.82015}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 124, 'time_epoch': 0.66791, 'loss': 0.53034744, 'lr': 0, 'params': 13807345, 'time_iter': 0.04453, 'mae': 0.53035, 'r2': 0.71657, 'spearmanr': 0.84663, 'mse': 0.56071, 'rmse': 0.74881}\n",
            "> Epoch 124: took 16.4s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 125, 'time_epoch': 13.75025, 'eta': 331.60701, 'eta_hours': 0.09211, 'loss': 0.16219821, 'lr': 3.571e-05, 'params': 13807345, 'time_iter': 0.13481, 'mae': 0.1622, 'r2': 0.97409, 'spearmanr': 0.98712, 'mse': 0.05323, 'rmse': 0.23072}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 125, 'time_epoch': 2.02923, 'loss': 0.58457055, 'lr': 0, 'params': 13807345, 'time_iter': 0.06997, 'mae': 0.58457, 'r2': 0.6708, 'spearmanr': 0.82817, 'mse': 0.66693, 'rmse': 0.81666}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 125, 'time_epoch': 0.70646, 'loss': 0.53129902, 'lr': 0, 'params': 13807345, 'time_iter': 0.0471, 'mae': 0.5313, 'r2': 0.7173, 'spearmanr': 0.84463, 'mse': 0.55928, 'rmse': 0.74785}\n",
            "> Epoch 125: took 16.5s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 126, 'time_epoch': 12.56163, 'eta': 317.56271, 'eta_hours': 0.08821, 'loss': 0.1650917, 'lr': 3.429e-05, 'params': 13807345, 'time_iter': 0.12315, 'mae': 0.16509, 'r2': 0.9731, 'spearmanr': 0.98703, 'mse': 0.05526, 'rmse': 0.23508}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 126, 'time_epoch': 1.97965, 'loss': 0.58108502, 'lr': 0, 'params': 13807345, 'time_iter': 0.06826, 'mae': 0.58109, 'r2': 0.67302, 'spearmanr': 0.82949, 'mse': 0.66245, 'rmse': 0.81391}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 126, 'time_epoch': 1.04578, 'loss': 0.5362891, 'lr': 0, 'params': 13807345, 'time_iter': 0.06972, 'mae': 0.53629, 'r2': 0.71157, 'spearmanr': 0.84316, 'mse': 0.57061, 'rmse': 0.75539}\n",
            "> Epoch 126: took 15.6s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 127, 'time_epoch': 12.46458, 'eta': 303.52489, 'eta_hours': 0.08431, 'loss': 0.15571192, 'lr': 3.286e-05, 'params': 13807345, 'time_iter': 0.1222, 'mae': 0.15571, 'r2': 0.97645, 'spearmanr': 0.98852, 'mse': 0.04838, 'rmse': 0.21996}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 127, 'time_epoch': 1.77133, 'loss': 0.57923406, 'lr': 0, 'params': 13807345, 'time_iter': 0.06108, 'mae': 0.57923, 'r2': 0.67546, 'spearmanr': 0.83011, 'mse': 0.65749, 'rmse': 0.81086}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 127, 'time_epoch': 0.91241, 'loss': 0.54250579, 'lr': 0, 'params': 13807345, 'time_iter': 0.06083, 'mae': 0.54251, 'r2': 0.7066, 'spearmanr': 0.84026, 'mse': 0.58045, 'rmse': 0.76187}\n",
            "> Epoch 127: took 15.2s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 128, 'time_epoch': 13.90784, 'eta': 289.74642, 'eta_hours': 0.08049, 'loss': 0.15326558, 'lr': 3.143e-05, 'params': 13807345, 'time_iter': 0.13635, 'mae': 0.15327, 'r2': 0.97718, 'spearmanr': 0.98904, 'mse': 0.04689, 'rmse': 0.21654}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 128, 'time_epoch': 1.28919, 'loss': 0.57916607, 'lr': 0, 'params': 13807345, 'time_iter': 0.04445, 'mae': 0.57917, 'r2': 0.67277, 'spearmanr': 0.83134, 'mse': 0.66294, 'rmse': 0.81421}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 128, 'time_epoch': 0.69921, 'loss': 0.53574719, 'lr': 0, 'params': 13807345, 'time_iter': 0.04661, 'mae': 0.53575, 'r2': 0.71042, 'spearmanr': 0.84343, 'mse': 0.57288, 'rmse': 0.75689}\n",
            "> Epoch 128: took 15.9s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 129, 'time_epoch': 14.31926, 'eta': 276.02925, 'eta_hours': 0.07667, 'loss': 0.15628739, 'lr': 3e-05, 'params': 13807345, 'time_iter': 0.14038, 'mae': 0.15629, 'r2': 0.97623, 'spearmanr': 0.98835, 'mse': 0.04884, 'rmse': 0.221}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 129, 'time_epoch': 1.32747, 'loss': 0.58029096, 'lr': 0, 'params': 13807345, 'time_iter': 0.04577, 'mae': 0.58029, 'r2': 0.67491, 'spearmanr': 0.83066, 'mse': 0.6586, 'rmse': 0.81154}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 129, 'time_epoch': 0.69104, 'loss': 0.53832125, 'lr': 0, 'params': 13807345, 'time_iter': 0.04607, 'mae': 0.53832, 'r2': 0.70734, 'spearmanr': 0.84263, 'mse': 0.57899, 'rmse': 0.76091}\n",
            "> Epoch 129: took 16.4s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 130, 'time_epoch': 14.42523, 'eta': 262.31826, 'eta_hours': 0.07287, 'loss': 0.15621085, 'lr': 2.857e-05, 'params': 13807345, 'time_iter': 0.14142, 'mae': 0.15621, 'r2': 0.97615, 'spearmanr': 0.98819, 'mse': 0.049, 'rmse': 0.22136}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 130, 'time_epoch': 1.34262, 'loss': 0.58539235, 'lr': 0, 'params': 13807345, 'time_iter': 0.0463, 'mae': 0.58539, 'r2': 0.66874, 'spearmanr': 0.82802, 'mse': 0.6711, 'rmse': 0.81921}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 130, 'time_epoch': 0.68335, 'loss': 0.53741701, 'lr': 0, 'params': 13807345, 'time_iter': 0.04556, 'mae': 0.53742, 'r2': 0.7067, 'spearmanr': 0.84071, 'mse': 0.58026, 'rmse': 0.76174}\n",
            "> Epoch 130: took 16.5s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 131, 'time_epoch': 14.48704, 'eta': 248.60487, 'eta_hours': 0.06906, 'loss': 0.15566152, 'lr': 2.714e-05, 'params': 13807345, 'time_iter': 0.14203, 'mae': 0.15566, 'r2': 0.97644, 'spearmanr': 0.98832, 'mse': 0.0484, 'rmse': 0.21999}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 131, 'time_epoch': 1.31504, 'loss': 0.57885737, 'lr': 0, 'params': 13807345, 'time_iter': 0.04535, 'mae': 0.57886, 'r2': 0.6737, 'spearmanr': 0.8301, 'mse': 0.66106, 'rmse': 0.81305}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 131, 'time_epoch': 0.67496, 'loss': 0.53730897, 'lr': 0, 'params': 13807345, 'time_iter': 0.045, 'mae': 0.53731, 'r2': 0.71146, 'spearmanr': 0.8444, 'mse': 0.57083, 'rmse': 0.75553}\n",
            "> Epoch 131: took 16.5s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 132, 'time_epoch': 14.72344, 'eta': 234.91007, 'eta_hours': 0.06525, 'loss': 0.15865156, 'lr': 2.571e-05, 'params': 13807345, 'time_iter': 0.14435, 'mae': 0.15865, 'r2': 0.97608, 'spearmanr': 0.98821, 'mse': 0.04914, 'rmse': 0.22168}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 132, 'time_epoch': 1.36233, 'loss': 0.58067558, 'lr': 0, 'params': 13807345, 'time_iter': 0.04698, 'mae': 0.58068, 'r2': 0.67444, 'spearmanr': 0.83062, 'mse': 0.65957, 'rmse': 0.81214}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 132, 'time_epoch': 0.70592, 'loss': 0.54027003, 'lr': 0, 'params': 13807345, 'time_iter': 0.04706, 'mae': 0.54027, 'r2': 0.71186, 'spearmanr': 0.84317, 'mse': 0.57003, 'rmse': 0.75501}\n",
            "> Epoch 132: took 16.8s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 133, 'time_epoch': 13.67435, 'eta': 221.07465, 'eta_hours': 0.06141, 'loss': 0.15638974, 'lr': 2.429e-05, 'params': 13807345, 'time_iter': 0.13406, 'mae': 0.15639, 'r2': 0.97603, 'spearmanr': 0.98825, 'mse': 0.04924, 'rmse': 0.22191}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 133, 'time_epoch': 2.078, 'loss': 0.57763866, 'lr': 0, 'params': 13807345, 'time_iter': 0.07166, 'mae': 0.57764, 'r2': 0.67732, 'spearmanr': 0.83306, 'mse': 0.65372, 'rmse': 0.80853}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 133, 'time_epoch': 1.09376, 'loss': 0.53766624, 'lr': 0, 'params': 13807345, 'time_iter': 0.07292, 'mae': 0.53767, 'r2': 0.71296, 'spearmanr': 0.84511, 'mse': 0.56786, 'rmse': 0.75357}\n",
            "> Epoch 133: took 16.9s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 134, 'time_epoch': 12.75108, 'eta': 207.13903, 'eta_hours': 0.05754, 'loss': 0.15446147, 'lr': 2.286e-05, 'params': 13807345, 'time_iter': 0.12501, 'mae': 0.15446, 'r2': 0.97729, 'spearmanr': 0.98882, 'mse': 0.04666, 'rmse': 0.21601}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 134, 'time_epoch': 1.90973, 'loss': 0.58013762, 'lr': 0, 'params': 13807345, 'time_iter': 0.06585, 'mae': 0.58014, 'r2': 0.6739, 'spearmanr': 0.82889, 'mse': 0.66065, 'rmse': 0.8128}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 134, 'time_epoch': 1.07298, 'loss': 0.5375673, 'lr': 0, 'params': 13807345, 'time_iter': 0.07153, 'mae': 0.53757, 'r2': 0.71207, 'spearmanr': 0.84297, 'mse': 0.56963, 'rmse': 0.75474}\n",
            "> Epoch 134: took 15.8s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 135, 'time_epoch': 12.78787, 'eta': 193.22462, 'eta_hours': 0.05367, 'loss': 0.15734256, 'lr': 2.143e-05, 'params': 13807345, 'time_iter': 0.12537, 'mae': 0.15734, 'r2': 0.97676, 'spearmanr': 0.98853, 'mse': 0.04775, 'rmse': 0.21851}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 135, 'time_epoch': 1.73716, 'loss': 0.57994687, 'lr': 0, 'params': 13807345, 'time_iter': 0.0599, 'mae': 0.57995, 'r2': 0.67545, 'spearmanr': 0.83113, 'mse': 0.65751, 'rmse': 0.81087}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 135, 'time_epoch': 0.95581, 'loss': 0.5393509, 'lr': 0, 'params': 13807345, 'time_iter': 0.06372, 'mae': 0.53935, 'r2': 0.70912, 'spearmanr': 0.843, 'mse': 0.57546, 'rmse': 0.75859}\n",
            "> Epoch 135: took 15.5s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 136, 'time_epoch': 13.98606, 'eta': 179.44035, 'eta_hours': 0.04984, 'loss': 0.15562028, 'lr': 2e-05, 'params': 13807345, 'time_iter': 0.13712, 'mae': 0.15562, 'r2': 0.97593, 'spearmanr': 0.98802, 'mse': 0.04945, 'rmse': 0.22238}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 136, 'time_epoch': 1.31895, 'loss': 0.58328541, 'lr': 0, 'params': 13807345, 'time_iter': 0.04548, 'mae': 0.58329, 'r2': 0.66901, 'spearmanr': 0.82858, 'mse': 0.67057, 'rmse': 0.81888}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 136, 'time_epoch': 0.67583, 'loss': 0.54264093, 'lr': 0, 'params': 13807345, 'time_iter': 0.04506, 'mae': 0.54264, 'r2': 0.70745, 'spearmanr': 0.84249, 'mse': 0.57877, 'rmse': 0.76077}\n",
            "> Epoch 136: took 16.0s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 137, 'time_epoch': 14.46145, 'eta': 165.69449, 'eta_hours': 0.04603, 'loss': 0.15391308, 'lr': 1.857e-05, 'params': 13807345, 'time_iter': 0.14178, 'mae': 0.15391, 'r2': 0.9775, 'spearmanr': 0.98913, 'mse': 0.04622, 'rmse': 0.215}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 137, 'time_epoch': 1.33045, 'loss': 0.58469011, 'lr': 0, 'params': 13807345, 'time_iter': 0.04588, 'mae': 0.58469, 'r2': 0.66957, 'spearmanr': 0.82845, 'mse': 0.66943, 'rmse': 0.81819}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 137, 'time_epoch': 0.71454, 'loss': 0.5392757, 'lr': 0, 'params': 13807345, 'time_iter': 0.04764, 'mae': 0.53928, 'r2': 0.70957, 'spearmanr': 0.84386, 'mse': 0.57457, 'rmse': 0.758}\n",
            "> Epoch 137: took 16.5s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 138, 'time_epoch': 14.39951, 'eta': 151.93344, 'eta_hours': 0.0422, 'loss': 0.15097306, 'lr': 1.714e-05, 'params': 13807345, 'time_iter': 0.14117, 'mae': 0.15097, 'r2': 0.97775, 'spearmanr': 0.98924, 'mse': 0.04571, 'rmse': 0.21379}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 138, 'time_epoch': 1.33136, 'loss': 0.58340156, 'lr': 0, 'params': 13807345, 'time_iter': 0.04591, 'mae': 0.5834, 'r2': 0.67082, 'spearmanr': 0.82893, 'mse': 0.66688, 'rmse': 0.81663}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 138, 'time_epoch': 0.69381, 'loss': 0.53671828, 'lr': 0, 'params': 13807345, 'time_iter': 0.04625, 'mae': 0.53672, 'r2': 0.70933, 'spearmanr': 0.84306, 'mse': 0.57505, 'rmse': 0.75832}\n",
            "> Epoch 138: took 16.5s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 139, 'time_epoch': 14.40053, 'eta': 138.16333, 'eta_hours': 0.03838, 'loss': 0.15435451, 'lr': 1.571e-05, 'params': 13807345, 'time_iter': 0.14118, 'mae': 0.15435, 'r2': 0.9765, 'spearmanr': 0.98848, 'mse': 0.04828, 'rmse': 0.21972}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 139, 'time_epoch': 1.37348, 'loss': 0.5865063, 'lr': 0, 'params': 13807345, 'time_iter': 0.04736, 'mae': 0.58651, 'r2': 0.66649, 'spearmanr': 0.82917, 'mse': 0.67567, 'rmse': 0.82199}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 139, 'time_epoch': 0.70641, 'loss': 0.5427965, 'lr': 0, 'params': 13807345, 'time_iter': 0.04709, 'mae': 0.5428, 'r2': 0.70649, 'spearmanr': 0.84334, 'mse': 0.58067, 'rmse': 0.76202}\n",
            "> Epoch 139: took 16.5s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 140, 'time_epoch': 14.31848, 'eta': 124.37905, 'eta_hours': 0.03455, 'loss': 0.15044829, 'lr': 1.429e-05, 'params': 13807345, 'time_iter': 0.14038, 'mae': 0.15045, 'r2': 0.97787, 'spearmanr': 0.98908, 'mse': 0.04547, 'rmse': 0.21324}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 140, 'time_epoch': 1.61881, 'loss': 0.58455613, 'lr': 0, 'params': 13807345, 'time_iter': 0.05582, 'mae': 0.58456, 'r2': 0.66967, 'spearmanr': 0.82908, 'mse': 0.66923, 'rmse': 0.81806}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 140, 'time_epoch': 0.69516, 'loss': 0.53941943, 'lr': 0, 'params': 13807345, 'time_iter': 0.04634, 'mae': 0.53942, 'r2': 0.70925, 'spearmanr': 0.84299, 'mse': 0.5752, 'rmse': 0.75842}\n",
            "> Epoch 140: took 16.7s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 141, 'time_epoch': 13.01348, 'eta': 110.51373, 'eta_hours': 0.0307, 'loss': 0.14968551, 'lr': 1.286e-05, 'params': 13807345, 'time_iter': 0.12758, 'mae': 0.14969, 'r2': 0.97799, 'spearmanr': 0.9889, 'mse': 0.04523, 'rmse': 0.21266}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 141, 'time_epoch': 1.98936, 'loss': 0.58169494, 'lr': 0, 'params': 13807345, 'time_iter': 0.0686, 'mae': 0.58169, 'r2': 0.672, 'spearmanr': 0.82982, 'mse': 0.66451, 'rmse': 0.81517}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 141, 'time_epoch': 1.06278, 'loss': 0.54011765, 'lr': 0, 'params': 13807345, 'time_iter': 0.07085, 'mae': 0.54012, 'r2': 0.70979, 'spearmanr': 0.8428, 'mse': 0.57414, 'rmse': 0.75772}\n",
            "> Epoch 141: took 16.1s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 142, 'time_epoch': 12.41781, 'eta': 96.63115, 'eta_hours': 0.02684, 'loss': 0.1486745, 'lr': 1.143e-05, 'params': 13807345, 'time_iter': 0.12174, 'mae': 0.14867, 'r2': 0.97803, 'spearmanr': 0.98913, 'mse': 0.04514, 'rmse': 0.21245}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 142, 'time_epoch': 1.75691, 'loss': 0.58069608, 'lr': 0, 'params': 13807345, 'time_iter': 0.06058, 'mae': 0.5807, 'r2': 0.67132, 'spearmanr': 0.83088, 'mse': 0.66588, 'rmse': 0.81601}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 142, 'time_epoch': 0.95894, 'loss': 0.54210713, 'lr': 0, 'params': 13807345, 'time_iter': 0.06393, 'mae': 0.54211, 'r2': 0.70789, 'spearmanr': 0.84419, 'mse': 0.5779, 'rmse': 0.7602}\n",
            "> Epoch 142: took 15.2s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 143, 'time_epoch': 13.25072, 'eta': 82.80363, 'eta_hours': 0.023, 'loss': 0.15142726, 'lr': 1e-05, 'params': 13807345, 'time_iter': 0.12991, 'mae': 0.15143, 'r2': 0.97829, 'spearmanr': 0.98941, 'mse': 0.04459, 'rmse': 0.21117}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 143, 'time_epoch': 1.36205, 'loss': 0.58207919, 'lr': 0, 'params': 13807345, 'time_iter': 0.04697, 'mae': 0.58208, 'r2': 0.67022, 'spearmanr': 0.82914, 'mse': 0.66812, 'rmse': 0.81738}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 143, 'time_epoch': 0.86878, 'loss': 0.53968855, 'lr': 0, 'params': 13807345, 'time_iter': 0.05792, 'mae': 0.53969, 'r2': 0.70802, 'spearmanr': 0.84351, 'mse': 0.57764, 'rmse': 0.76003}\n",
            "> Epoch 143: took 15.5s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 144, 'time_epoch': 14.31535, 'eta': 69.02078, 'eta_hours': 0.01917, 'loss': 0.14625039, 'lr': 8.57e-06, 'params': 13807345, 'time_iter': 0.14035, 'mae': 0.14625, 'r2': 0.97911, 'spearmanr': 0.98961, 'mse': 0.04291, 'rmse': 0.20716}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 144, 'time_epoch': 1.30742, 'loss': 0.58170152, 'lr': 0, 'params': 13807345, 'time_iter': 0.04508, 'mae': 0.5817, 'r2': 0.67139, 'spearmanr': 0.82918, 'mse': 0.66573, 'rmse': 0.81592}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 144, 'time_epoch': 0.71899, 'loss': 0.54215587, 'lr': 0, 'params': 13807345, 'time_iter': 0.04793, 'mae': 0.54216, 'r2': 0.70957, 'spearmanr': 0.84324, 'mse': 0.57457, 'rmse': 0.75801}\n",
            "> Epoch 144: took 16.4s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 145, 'time_epoch': 14.52684, 'eta': 55.23642, 'eta_hours': 0.01534, 'loss': 0.14858724, 'lr': 7.14e-06, 'params': 13807345, 'time_iter': 0.14242, 'mae': 0.14859, 'r2': 0.9782, 'spearmanr': 0.9894, 'mse': 0.04479, 'rmse': 0.21163}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 145, 'time_epoch': 1.34082, 'loss': 0.58403339, 'lr': 0, 'params': 13807345, 'time_iter': 0.04624, 'mae': 0.58403, 'r2': 0.66974, 'spearmanr': 0.82817, 'mse': 0.66908, 'rmse': 0.81797}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 145, 'time_epoch': 0.69044, 'loss': 0.53696891, 'lr': 0, 'params': 13807345, 'time_iter': 0.04603, 'mae': 0.53697, 'r2': 0.70995, 'spearmanr': 0.84419, 'mse': 0.57382, 'rmse': 0.75751}\n",
            "> Epoch 145: took 16.6s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 146, 'time_epoch': 14.40304, 'eta': 41.43944, 'eta_hours': 0.01151, 'loss': 0.14921261, 'lr': 5.71e-06, 'params': 13807345, 'time_iter': 0.14121, 'mae': 0.14921, 'r2': 0.97815, 'spearmanr': 0.98929, 'mse': 0.04489, 'rmse': 0.21188}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 146, 'time_epoch': 1.33517, 'loss': 0.58540003, 'lr': 0, 'params': 13807345, 'time_iter': 0.04604, 'mae': 0.5854, 'r2': 0.66831, 'spearmanr': 0.82905, 'mse': 0.67198, 'rmse': 0.81974}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 146, 'time_epoch': 0.68743, 'loss': 0.53912205, 'lr': 0, 'params': 13807345, 'time_iter': 0.04583, 'mae': 0.53912, 'r2': 0.71049, 'spearmanr': 0.84548, 'mse': 0.57276, 'rmse': 0.75681}\n",
            "> Epoch 146: took 16.5s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 147, 'time_epoch': 14.34615, 'eta': 27.63349, 'eta_hours': 0.00768, 'loss': 0.14857418, 'lr': 4.29e-06, 'params': 13807345, 'time_iter': 0.14065, 'mae': 0.14857, 'r2': 0.97806, 'spearmanr': 0.98939, 'mse': 0.04508, 'rmse': 0.21231}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 147, 'time_epoch': 1.31667, 'loss': 0.58145338, 'lr': 0, 'params': 13807345, 'time_iter': 0.0454, 'mae': 0.58145, 'r2': 0.672, 'spearmanr': 0.82976, 'mse': 0.6645, 'rmse': 0.81517}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 147, 'time_epoch': 0.68478, 'loss': 0.5397559, 'lr': 0, 'params': 13807345, 'time_iter': 0.04565, 'mae': 0.53976, 'r2': 0.70825, 'spearmanr': 0.84395, 'mse': 0.57718, 'rmse': 0.75973}\n",
            "> Epoch 147: took 16.4s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 148, 'time_epoch': 13.64318, 'eta': 13.81558, 'eta_hours': 0.00384, 'loss': 0.14782206, 'lr': 2.86e-06, 'params': 13807345, 'time_iter': 0.13376, 'mae': 0.14782, 'r2': 0.97895, 'spearmanr': 0.98969, 'mse': 0.04324, 'rmse': 0.20795}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 148, 'time_epoch': 2.00165, 'loss': 0.5836699, 'lr': 0, 'params': 13807345, 'time_iter': 0.06902, 'mae': 0.58367, 'r2': 0.66922, 'spearmanr': 0.82773, 'mse': 0.67013, 'rmse': 0.81862}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 148, 'time_epoch': 0.92552, 'loss': 0.53942594, 'lr': 0, 'params': 13807345, 'time_iter': 0.0617, 'mae': 0.53943, 'r2': 0.70948, 'spearmanr': 0.84369, 'mse': 0.57475, 'rmse': 0.75812}\n",
            "> Epoch 148: took 16.6s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "train: {'epoch': 149, 'time_epoch': 12.84988, 'eta': 0.0, 'eta_hours': 0.0, 'loss': 0.14483224, 'lr': 1.43e-06, 'params': 13807345, 'time_iter': 0.12598, 'mae': 0.14483, 'r2': 0.97972, 'spearmanr': 0.9903, 'mse': 0.04167, 'rmse': 0.20414}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "val: {'epoch': 149, 'time_epoch': 1.99501, 'loss': 0.5845372, 'lr': 0, 'params': 13807345, 'time_iter': 0.06879, 'mae': 0.58454, 'r2': 0.66764, 'spearmanr': 0.82846, 'mse': 0.67334, 'rmse': 0.82057}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._native_multi_head_attention(\n",
            "test: {'epoch': 149, 'time_epoch': 1.05781, 'loss': 0.53957407, 'lr': 0, 'params': 13807345, 'time_iter': 0.07052, 'mae': 0.53957, 'r2': 0.70972, 'spearmanr': 0.8445, 'mse': 0.57427, 'rmse': 0.75781}\n",
            "> Epoch 149: took 16.0s (avg 16.2s) | Best so far: epoch 56\ttrain_loss: 0.2600 train_mae: 0.2600\tval_loss: 0.5753 val_mae: 0.5753\ttest_loss: 0.5388 test_mae: 0.5388\n",
            "Avg time per epoch: 16.18s\n",
            "Total train loop time: 0.67h\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best/epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–†â–†â–†â–†â–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best/test_loss â–ˆâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best/test_mae â–ˆâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  best/train_loss â–ˆâ–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   best/train_mae â–ˆâ–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    best/val_loss â–ˆâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best/val_mae â–ˆâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       test/epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test/loss â–ˆâ–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          test/lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         test/mae â–ˆâ–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         test/mse â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      test/params â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          test/r2 â–â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test/rmse â–ˆâ–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   test/spearmanr â–â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test/time_epoch â–‚â–â–…â–â–…â–â–‚â–â–‚â–…â–…â–â–â–â–â–…â–‚â–‡â–â–‡â–‚â–â–â–‡â–â–‡â–â–‡â–â–‡â–â–â–â–‚â–â–ˆâ–‚â–‡â–â–‡\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   test/time_iter â–‚â–â–…â–â–…â–â–‚â–â–‚â–…â–…â–â–â–â–â–…â–‚â–‡â–â–‡â–‚â–â–â–‡â–â–‡â–â–‡â–â–‡â–â–â–â–‚â–â–ˆâ–‚â–‡â–â–‡\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        train/eta â–ˆâ–ˆâ–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  train/eta_hours â–ˆâ–ˆâ–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       train/loss â–ˆâ–…â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/lr â–‚â–ƒâ–†â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        train/mae â–ˆâ–…â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        train/mse â–ˆâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train/params â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/r2 â–â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       train/rmse â–ˆâ–…â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  train/spearmanr â–â–…â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/time_epoch â–‡â–ˆâ–â–ˆâ–‚â–‡â–„â–†â–†â–…â–‚â–‡â–…â–‡â–†â–„â–ˆâ–ƒâ–ˆâ–â–†â–…â–‡â–ƒâ–‡â–â–‡â–‚â–ˆâ–‚â–†â–†â–‡â–…â–‡â–…â–ˆâ–ƒâ–ˆâ–‚\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  train/time_iter â–‡â–ˆâ–â–ˆâ–‚â–‡â–„â–†â–†â–…â–‚â–‡â–…â–‡â–†â–„â–ˆâ–ƒâ–ˆâ–â–†â–…â–‡â–ƒâ–‡â–â–‡â–‚â–ˆâ–‚â–†â–†â–‡â–…â–‡â–…â–ˆâ–ƒâ–ˆâ–‚\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val/epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val/loss â–ˆâ–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          val/mae â–ˆâ–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          val/mse â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val/params â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/r2 â–â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         val/rmse â–ˆâ–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    val/spearmanr â–â–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val/time_epoch â–â–â–…â–â–ƒâ–â–â–ƒâ–‚â–‡â–„â–â–â–„â–â–‡â–â–ˆâ–â–†â–â–‡â–â–‡â–â–‡â–â–‡â–â–†â–â–…â–â–ˆâ–â–ˆâ–â–‡â–â–‡\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    val/time_iter â–â–â–…â–â–ƒâ–â–â–ƒâ–‚â–‡â–„â–â–â–„â–â–‡â–â–ˆâ–â–†â–â–‡â–â–‡â–â–‡â–â–‡â–â–†â–â–…â–â–ˆâ–â–ˆâ–â–‡â–â–‡\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          best/epoch 56\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best/test_loss 0.53884\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best/test_mae 0.53884\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best/train_loss 0.26\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best/train_mae 0.26\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best/val_loss 0.57528\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        best/val_mae 0.57528\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      best_test_perf 0.53884\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     best_train_perf 0.26\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       best_val_perf 0.57528\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: full_epoch_time_avg 16.18315\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: full_epoch_time_sum 2427.47285\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          test/epoch 149\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           test/loss 0.53957\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             test/lr 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            test/mae 0.53957\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            test/mse 0.57427\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         test/params 13807345\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             test/r2 0.70972\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           test/rmse 0.75781\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      test/spearmanr 0.8445\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     test/time_epoch 1.05781\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      test/time_iter 0.07052\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/epoch 149\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/eta 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train/eta_hours 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/loss 0.14483\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/lr 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/mae 0.14483\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           train/mse 0.04167\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        train/params 13807345\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/r2 0.97972\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/rmse 0.20414\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train/spearmanr 0.9903\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    train/time_epoch 12.84988\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train/time_iter 0.12598\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/epoch 149\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            val/loss 0.58454\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val/lr 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/mae 0.58454\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val/mse 0.67334\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          val/params 13807345\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val/r2 0.66764\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            val/rmse 0.82057\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val/spearmanr 0.82846\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val/time_epoch 1.99501\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val/time_iter 0.06879\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33m-EGFR-EGFR_compounds_lipinsky.csv.GPS.CustomGatedGCN+Transformer+RWSE.r0\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/tz545/EGFR/runs/e0uhatbt\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ï¸âš¡ View job at \u001b[34m\u001b[4mhttps://wandb.ai/tz545/EGFR/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjk0Nzc1MDAz/version_details/v0\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230903_202925-e0uhatbt/logs\u001b[0m\n",
            "Task done, results saved in results/EGFR-finetune/0\n",
            "56\n",
            "{'epoch': 56, 'time_epoch': 14.61525, 'eta': 1290.10131, 'eta_hours': 0.35836, 'loss': 0.26000255, 'lr': 0.00013429, 'params': 13807345, 'time_iter': 0.14329, 'mae': 0.26, 'r2': 0.93146, 'spearmanr': 0.96583, 'mse': 0.14081, 'rmse': 0.37524}\n",
            "{'epoch': 56, 'time_epoch': 1.3176, 'loss': 0.57528366, 'lr': 0, 'params': 13807345, 'time_iter': 0.04543, 'mae': 0.57528, 'r2': 0.67742, 'spearmanr': 0.83183, 'mse': 0.65353, 'rmse': 0.80841}\n",
            "{'epoch': 56, 'time_epoch': 0.69048, 'loss': 0.53884413, 'lr': 0, 'params': 13807345, 'time_iter': 0.04603, 'mae': 0.53884, 'r2': 0.71397, 'spearmanr': 0.84825, 'mse': 0.56587, 'rmse': 0.75224}\n",
            "Results aggregated across runs saved in results/EGFR-finetune/agg\n",
            "[*] All done: 2023-09-03 21:10:10.790913\n"
          ]
        }
      ]
    }
  ]
}